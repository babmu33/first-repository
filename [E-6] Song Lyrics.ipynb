{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bizarre-victory",
   "metadata": {},
   "source": [
    "## 7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-mexico",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "#$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-ranch",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기    \n",
    "\n",
    "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "helpful-benjamin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Another one', 'We The Best music', \"DJ Khaled I don't know if you could take it\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conscious-boutique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffEighteen years eighteen years', 'She got one of your kids got you for eighteen years', 'I know somebody paying child support for one of his kids', 'His baby mama car and crib is bigger than his', 'You will see him on TV any given Sunday', 'Win the Super Bowl and drive off in a Hyundai', 'She was supposed to buy your shorty Tyco with your money', 'She went to the doctor got lipo with your money', 'She walking around looking like Michael with your money']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    " \n",
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/Kanye_West.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-peace",
   "metadata": {},
   "source": [
    "#### 데이터를 살펴보니 노래 가사인듯 보이네요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adolescent-corrections",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Eighteen years eighteen years\n",
      "She got one of your kids got you for eighteen years\n",
      "I know somebody paying child support for one of his kids\n",
      "His baby mama car and crib is bigger than his\n",
      "You will see him on TV any given Sunday\n",
      "Win the Super Bowl and drive off in a Hyundai\n",
      "She was supposed to buy your shorty Tyco with your money\n",
      "She went to the doctor got lipo with your money\n",
      "She walking around looking like Michael with your money\n",
      "Shouldve got that insured Geico for your money\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-palace",
   "metadata": {},
   "source": [
    "#### 우리가 원하는 문장이 성공적으로 출력되었습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-ballot",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제   \n",
    "\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!   \n",
    "\n",
    "\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.   \n",
    "\n",
    "\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.   \n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dimensional-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-library",
   "metadata": {},
   "source": [
    "#### 문장부호를 삭제하고, 대문자를 소문자로 변환하고, 특수문자를 모드 제거하는 지저분한 문장을 깨끗하게 정제해 주는 함수가 완성되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "settled-logic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> eighteen years eighteen years <end>',\n",
       " '<start> she got one of your kids got you for eighteen years <end>',\n",
       " '<start> i know somebody paying child support for one of his kids <end>',\n",
       " '<start> his baby mama car and crib is bigger than his <end>',\n",
       " '<start> you will see him on tv any given sunday <end>',\n",
       " '<start> win the super bowl and drive off in a hyundai <end>',\n",
       " '<start> she was supposed to buy your shorty tyco with your money <end>',\n",
       " '<start> she went to the doctor got lipo with your money <end>',\n",
       " '<start> she walking around looking like michael with your money <end>',\n",
       " '<start> shouldve got that insured geico for your money <end>']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue \n",
    "    # if len(corpus) >= 15: del    \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "          \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-permit",
   "metadata": {},
   "source": [
    "#### 소스문장과 타켓문장을 생성하기 위한 단계로 정제함수를 통해  완벽하게 데이터를 준비 했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-sally",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리   \n",
    "\n",
    "\n",
    "훈련 데이터와 평가 데이터를 분리하세요!   \n",
    "\n",
    "\n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc_train, enc_val, dec_train, dec_val = <코드 작성>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "serious-deficit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 931 243 ...   0   0   0]\n",
      " [  2  25  26 ...   0   0   0]\n",
      " [  2   5  29 ...   0   0   0]\n",
      " ...\n",
      " [  2  24   6 ...   0   0   0]\n",
      " [  2  10 217 ...  13   3   0]\n",
      " [  2  28  12 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f68d027f490>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=16, truncating='post',padding='post')  \n",
    "                                    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-orbit",
   "metadata": {},
   "source": [
    "####  토큰화를 진행 하고 토큰개수 15개가 넘는 단어들을 제거하고 싶었지만 제 짧은 실력으로는 아무리 if문을 써보고 이것 저것을 다 해봐도 해결할수가 없었지만  방법이 없어서 그냥 패딩이 과도하게 들어가더라도 진행하려 했는데 막상 하다보니 신경이 쓰여서 한참 구글링을 해보니 패딩을 주는 과정에서도 데이터를 잘라낼수 있는 방법이 있어서 한번 시도해 봤더니 보기좋게 삭제가 되었네요. 기분좋습니다~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "active-gravity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : the\n",
      "5 : i\n",
      "6 : you\n",
      "7 : a\n",
      "8 : and\n",
      "9 : to\n",
      "10 : my\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-testimony",
   "metadata": {},
   "source": [
    "#### 단어 사전이 아주 잘 구축되어 있는걸 확인할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fatty-resort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 931 243 931 243   3   0   0   0   0   0   0   0   0   0]\n",
      "[931 243 931 243   3   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "sapphire-constant",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-8501bf55ad11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source Train:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target Train:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-going",
   "metadata": {},
   "source": [
    "#### 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dimensional-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 931 243 ...   0   0   0]\n",
      " [  2  25  26 ...   0   0   0]\n",
      " [  2   5  29 ...   0   0   0]\n",
      " ...\n",
      " [  2  24   6 ...   0   0   0]\n",
      " [  2  10 217 ...  13   3   0]\n",
      " [  2  28  12 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f684c7d2f90>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b7b13797454f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-break",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-evolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "effective-helicopter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-communication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-story",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-cookbook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-credit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "primary-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "motivated-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 12001), dtype=float32, numpy=\n",
       "array([[[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [-1.96843423e-04, -3.26531037e-04,  2.97538354e-04, ...,\n",
       "         -4.25766688e-04, -1.46763734e-04,  4.69094317e-04],\n",
       "        [-2.44447147e-04, -4.12672758e-04,  3.81603517e-04, ...,\n",
       "         -4.28438478e-04, -3.29473551e-04,  6.09909592e-04],\n",
       "        ...,\n",
       "        [-6.53210853e-04, -1.03181133e-04, -1.23922480e-03, ...,\n",
       "         -3.71217640e-04, -1.88861985e-03, -2.89449294e-04],\n",
       "        [-4.72459971e-04,  1.74342524e-04, -1.54902868e-03, ...,\n",
       "         -4.23955062e-04, -1.80421211e-03, -2.86192371e-04],\n",
       "        [-3.19634477e-04,  4.65171470e-04, -1.86414702e-03, ...,\n",
       "         -4.53660439e-04, -1.66629604e-03, -2.68252013e-04]],\n",
       "\n",
       "       [[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [-1.92641077e-04, -2.19940921e-06,  1.39465526e-04, ...,\n",
       "         -7.33739987e-04, -4.98024747e-04,  4.71497071e-04],\n",
       "        [-1.67216349e-04, -3.73788171e-05,  1.42264471e-04, ...,\n",
       "         -7.25091028e-04, -8.45533912e-04,  6.28425158e-04],\n",
       "        ...,\n",
       "        [ 4.20756318e-04,  1.07326568e-03, -1.93262508e-03, ...,\n",
       "         -1.08967779e-05, -1.57116447e-03, -3.34643992e-04],\n",
       "        [ 4.28472355e-04,  1.33980520e-03, -2.21872539e-03, ...,\n",
       "         -1.55621583e-05, -1.33554032e-03, -2.87965202e-04],\n",
       "        [ 4.18025535e-04,  1.57031522e-03, -2.47501442e-03, ...,\n",
       "         -5.74588876e-06, -1.09253800e-03, -2.35712127e-04]],\n",
       "\n",
       "       [[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [-1.37931871e-04, -1.60142969e-04,  2.25898460e-04, ...,\n",
       "         -7.95749307e-04, -1.33293885e-04,  4.13022412e-04],\n",
       "        [-1.78874194e-04, -2.16426361e-05,  2.14933520e-04, ...,\n",
       "         -1.03882921e-03, -1.75069217e-04,  7.47577113e-04],\n",
       "        ...,\n",
       "        [ 1.19557262e-05,  8.37423606e-04, -2.02371692e-03, ...,\n",
       "         -2.90494616e-04, -1.66208693e-03, -4.41111333e-04],\n",
       "        [ 5.48544267e-05,  1.07817666e-03, -2.29658070e-03, ...,\n",
       "         -3.07233626e-04, -1.46808475e-03, -4.15504997e-04],\n",
       "        [ 8.11979407e-05,  1.30909635e-03, -2.54683825e-03, ...,\n",
       "         -3.01580876e-04, -1.24959485e-03, -3.73216346e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [-5.30364341e-05, -1.14862603e-04,  3.07073293e-04, ...,\n",
       "         -5.84197696e-04, -2.21432696e-04,  4.91522253e-04],\n",
       "        [ 1.11584952e-04, -3.60662380e-04,  4.88835736e-04, ...,\n",
       "         -7.28552695e-04, -2.92563753e-04,  8.13649443e-04],\n",
       "        ...,\n",
       "        [-1.06303464e-03,  8.03700532e-04, -2.26265882e-04, ...,\n",
       "         -1.41452925e-04, -1.80794741e-03, -4.20997967e-04],\n",
       "        [-9.71713976e-04,  8.57111998e-04, -4.77169116e-04, ...,\n",
       "         -2.75213388e-04, -1.91505824e-03, -4.31898719e-04],\n",
       "        [-8.49717471e-04,  9.66582971e-04, -7.78617279e-04, ...,\n",
       "         -3.82879254e-04, -1.93657866e-03, -4.11469926e-04]],\n",
       "\n",
       "       [[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [-4.40604519e-04, -7.41216209e-05,  8.87320639e-05, ...,\n",
       "         -2.12385785e-04, -2.90614262e-04,  7.20492215e-04],\n",
       "        [-3.09453986e-04,  4.84470038e-05, -1.41036260e-04, ...,\n",
       "         -4.29426582e-04, -6.14077901e-04,  1.01289770e-03],\n",
       "        ...,\n",
       "        [ 1.21836187e-04,  1.45085913e-03, -1.22890982e-03, ...,\n",
       "         -2.80804932e-04, -1.33950089e-03,  5.60219632e-06],\n",
       "        [ 1.35424329e-04,  1.63720688e-03, -1.61342812e-03, ...,\n",
       "         -2.68640113e-04, -1.16180675e-03, -4.17014853e-05],\n",
       "        [ 1.41640368e-04,  1.80033839e-03, -1.96455396e-03, ...,\n",
       "         -2.41005298e-04, -9.63647675e-04, -6.42160740e-05]],\n",
       "\n",
       "       [[-1.13232825e-04, -1.79957497e-04,  1.87084021e-04, ...,\n",
       "         -3.26394103e-04, -8.45029208e-05,  2.28053948e-04],\n",
       "        [ 1.16568272e-05, -2.82353401e-04,  3.67943023e-04, ...,\n",
       "         -2.84207577e-04,  9.64478568e-06,  4.47978702e-04],\n",
       "        [-1.54775684e-04, -3.55583208e-04,  3.83863138e-04, ...,\n",
       "         -3.53065006e-05, -2.11678016e-05,  8.30480072e-04],\n",
       "        ...,\n",
       "        [-4.63332515e-04,  8.52693338e-05,  1.37123905e-04, ...,\n",
       "          7.38961244e-05, -1.77743915e-03, -3.28909518e-04],\n",
       "        [-3.62057734e-04,  3.52965377e-04, -3.24251014e-04, ...,\n",
       "         -1.63160439e-05, -1.72268040e-03, -3.79851146e-04],\n",
       "        [-2.71954690e-04,  6.26119785e-04, -7.89937680e-04, ...,\n",
       "         -8.34970415e-05, -1.61496142e-03, -3.94673727e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "labeled-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-float",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sticky-senior",
   "metadata": {},
   "source": [
    " 여기까지 올바르게 진행했을 경우, 아래 실행 결과를 확인할 수 있습니다.   \n",
    " \n",
    " \n",
    " \n",
    "print(\"Source Train:\", enc_train.shape)   \n",
    "print(\"Target Train:\", dec_train.shape)   \n",
    "\n",
    "\n",
    "out:   \n",
    "\n",
    "\n",
    "\n",
    "Source Train: (124960, 14)   \n",
    "Target Train: (124960, 14)   \n",
    "\n",
    "\n",
    "만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하세요! 만약 학습데이터 갯수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시한번 검토해 보시기를 권합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-french",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-restaurant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "smart-noise",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기   \n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)   \n",
    "\n",
    "\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#    from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-glory",
   "metadata": {},
   "source": [
    "데이터가 커서 훈련하는 데 시간이 제법 걸릴 겁니다. 여유를 가지고 작업하시면 좋아요 :)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-movement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-people",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-intensity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-carnival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-rider",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-paintball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-delhi",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
