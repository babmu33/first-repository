{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bizarre-victory",
   "metadata": {},
   "source": [
    "## 7. 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-mexico",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sixth-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "#$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-ranch",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 읽어오기    \n",
    "\n",
    "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpful-benjamin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Another one', 'We The Best music', \"DJ Khaled I don't know if you could take it\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adolescent-corrections",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another one\n",
      "We The Best music\n",
      "DJ Khaled I don't know if you could take it\n",
      "Know you wanna see me nakey, nakey, naked\n",
      "I wanna be your baby, baby, baby\n",
      "Spinning and it's wet just like it came from Maytag\n",
      "White girl wasted on that brown liquor\n",
      "When I get like this I can't be around you\n",
      "I'm too lit to dim down a notch\n",
      "'Cause I could name some thangs that I'm gon' do Wild, wild, wild\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-palace",
   "metadata": {},
   "source": [
    "#### 우리가 원하는 문장이 성공적으로 출력되었습니다.  데이터를 살펴보니 노래 가사인듯 보이네요. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-ballot",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 정제   \n",
    "\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!   \n",
    "\n",
    "\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.   \n",
    "\n",
    "\n",
    "\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래가사 작사하기에 어울리지 않을수도 있겠죠.   \n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권합니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dimensional-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-library",
   "metadata": {},
   "source": [
    "#### 문장부호를 삭제하고, 대문자를 소문자로 변환하고, 특수문자를 모드 제거하는 지저분한 문장을 깨끗하게 정제해 주는 함수가 완성되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "settled-logic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> another one <end>',\n",
       " '<start> we the best music <end>',\n",
       " '<start> dj khaled i don t know if you could take it <end>',\n",
       " '<start> know you wanna see me nakey , nakey , naked <end>',\n",
       " '<start> i wanna be your baby , baby , baby <end>',\n",
       " '<start> spinning and it s wet just like it came from maytag <end>',\n",
       " '<start> white girl wasted on that brown liquor <end>',\n",
       " '<start> when i get like this i can t be around you <end>',\n",
       " '<start> i m too lit to dim down a notch <end>',\n",
       " '<start> cause i could name some thangs that i m gon do wild , wild , wild <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue   \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "          \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-permit",
   "metadata": {},
   "source": [
    "#### 소스문장과 타켓문장을 생성하기 위한 단계로 정제함수를 통해  완벽하게 데이터를 준비 했습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-sally",
   "metadata": {},
   "source": [
    "### Step 4. 평가 데이터셋 분리   \n",
    "\n",
    "\n",
    "훈련 데이터와 평가 데이터를 분리하세요!   \n",
    "\n",
    "\n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20%를 평가 데이터셋으로 사용해 주세요!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serious-deficit",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  207   58 ...    0    0    0]\n",
      " [   2   23    6 ...    0    0    0]\n",
      " [   2  730  924 ...    3    0    0]\n",
      " ...\n",
      " [   2  211    3 ...    0    0    0]\n",
      " [   2  400    9 ...    0    0    0]\n",
      " [   2    9 1293 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fd399cba250>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, truncating='post', padding='post')  \n",
    "                                    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-orbit",
   "metadata": {},
   "source": [
    "####  토큰화를 진행 하고 토큰개수 15개가 넘는 단어들을 제거하고 싶었지만 제 짧은 실력으로는 아무리 if문을 써보고 이것 저것을 다 해봐도 해결할수가 없었지만  방법이 없어서 그냥 패딩이 과도하게 들어가더라도 진행하려 했는데 막상 하다보니 신경이 쓰여서 한참 구글링을 해보니 패딩을 주는 과정에서도 데이터를 잘라낼수 있는 방법이 있어서 한번 시도해 봤더니 보기좋게 삭제가 되었네요. 기분좋습니다~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fatty-resort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 207  58   3   0   0   0   0   0   0   0   0   0   0]\n",
      "[207  58   3   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-going",
   "metadata": {},
   "source": [
    "#### 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "active-gravity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-testimony",
   "metadata": {},
   "source": [
    "#### 단어 사전이 아주 잘 구축되어 있는걸 확인할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dimensional-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-simon",
   "metadata": {},
   "source": [
    "#### 트레인셋과 평가셋을 80:20으로 분리 했습니다. 그런데 불행히도 데이터수가 예시한 데이터 수를 훌쩍 넘어버리네요 ^^;; 더는 못하겠습니다. 그냥 진행 할께요. 여기 까지 오는데 3일 걸렸는데..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-noise",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 만들기   \n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)   \n",
    "\n",
    "\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "copyrighted-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-spotlight",
   "metadata": {},
   "source": [
    "#### NumPy 배열을 tf.data.Dataset으로 생성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "transsexual-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((300, 14), (300, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 300\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-processing",
   "metadata": {},
   "source": [
    "####  데이터 다듬기가 끝났습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "primary-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-cooking",
   "metadata": {},
   "source": [
    "####  1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있는 모델을 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "thousand-worse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 3.84078943e-04,  1.29547450e-04, -2.17329689e-05, ...,\n",
       "          1.46865394e-04,  1.91141444e-04,  2.76025257e-05],\n",
       "        [ 6.34591503e-04, -3.82865874e-05, -7.81534109e-05, ...,\n",
       "         -3.80671117e-05,  2.74315244e-04, -5.35688050e-05],\n",
       "        ...,\n",
       "        [ 8.07293865e-04, -6.49974027e-05, -7.55081652e-04, ...,\n",
       "          7.25414371e-04, -1.25252130e-03,  6.13332595e-05],\n",
       "        [ 6.67466433e-04, -2.18658286e-04, -7.20852753e-04, ...,\n",
       "          5.19172230e-04, -1.27041980e-03,  3.35922203e-04],\n",
       "        [ 5.67878305e-04, -2.09090998e-04, -9.01297899e-04, ...,\n",
       "          3.99915327e-04, -1.13194704e-03,  5.74321137e-04]],\n",
       "\n",
       "       [[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 6.44568703e-04,  1.62803495e-04, -4.73093329e-04, ...,\n",
       "          3.04838759e-04, -1.20815115e-04,  1.79094695e-05],\n",
       "        [ 8.41705187e-04, -5.23667040e-05, -6.94687944e-04, ...,\n",
       "          2.26318662e-04, -2.59344670e-04,  7.82143179e-05],\n",
       "        ...,\n",
       "        [ 4.12650668e-04,  6.44764747e-04, -2.40732319e-04, ...,\n",
       "         -9.99705517e-04, -3.03813373e-04,  1.04207534e-03],\n",
       "        [ 5.43147675e-04,  9.43721854e-04,  6.36726054e-06, ...,\n",
       "         -6.40124606e-04, -3.99629353e-04,  8.64996982e-04],\n",
       "        [ 6.44135289e-04,  1.24131120e-03, -1.70986139e-04, ...,\n",
       "         -1.34388465e-04, -5.81757922e-04,  5.33855578e-04]],\n",
       "\n",
       "       [[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 5.03514369e-04, -1.74009001e-05,  1.96970301e-04, ...,\n",
       "          1.03919119e-04, -1.78824503e-05,  1.44516516e-05],\n",
       "        [ 7.20828539e-04,  1.97032350e-05,  4.40015283e-04, ...,\n",
       "          2.34487481e-04, -3.87429318e-05,  2.95243110e-04],\n",
       "        ...,\n",
       "        [-6.66913111e-04,  1.37470174e-03,  3.08616727e-04, ...,\n",
       "         -2.14511994e-04,  5.08229423e-04,  4.77356662e-04],\n",
       "        [-6.65579922e-04,  1.24513370e-03,  1.71238262e-05, ...,\n",
       "         -1.48581894e-04,  5.57767635e-04,  3.10932082e-04],\n",
       "        [-7.66637560e-04,  1.05998013e-03, -7.86948876e-05, ...,\n",
       "         -4.10822031e-05,  8.33990925e-04,  1.89271523e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 4.42397752e-04,  2.32179416e-04, -5.09569654e-04, ...,\n",
       "          5.33992948e-04, -1.24070459e-04, -1.90859559e-04],\n",
       "        [ 6.07551192e-04,  3.04572022e-04, -7.60323310e-04, ...,\n",
       "          6.58749137e-04, -3.31332412e-04, -2.19095411e-04],\n",
       "        ...,\n",
       "        [-1.36469712e-03,  4.92276333e-04, -6.55174372e-04, ...,\n",
       "         -1.48407678e-04,  7.39370647e-04,  2.91740318e-04],\n",
       "        [-1.72166410e-03,  5.39384200e-04, -7.28782965e-04, ...,\n",
       "         -5.22241986e-04,  1.13849819e-03,  4.70334751e-04],\n",
       "        [-2.03907373e-03,  5.59523527e-04, -7.85735610e-04, ...,\n",
       "         -8.70040909e-04,  1.52447063e-03,  6.42467407e-04]],\n",
       "\n",
       "       [[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 3.91353969e-04,  1.66271609e-04, -3.91499168e-04, ...,\n",
       "          2.88683164e-04,  6.83996332e-05,  7.04298145e-05],\n",
       "        [ 7.02817168e-04,  2.14420197e-05, -6.24550856e-04, ...,\n",
       "          2.44690280e-04, -8.48101408e-05,  9.51885377e-05],\n",
       "        ...,\n",
       "        [-6.91818947e-04,  3.21454922e-04, -3.16546182e-04, ...,\n",
       "         -8.09733174e-04,  3.59998172e-04,  7.86285440e-04],\n",
       "        [-1.02679466e-03,  3.29198549e-04, -3.69171699e-04, ...,\n",
       "         -1.05428882e-03,  6.62440551e-04,  8.35901068e-04],\n",
       "        [-1.33612228e-03,  3.20801744e-04, -4.41738724e-04, ...,\n",
       "         -1.31440244e-03,  9.99498647e-04,  9.19541752e-04]],\n",
       "\n",
       "       [[ 2.96398764e-04,  8.54467580e-05, -9.99749900e-05, ...,\n",
       "          1.54611509e-04,  3.57719437e-05, -4.32251654e-05],\n",
       "        [ 5.35699830e-04,  3.93573253e-04,  1.10392095e-04, ...,\n",
       "         -3.90140689e-04,  2.86008930e-04, -3.28707305e-04],\n",
       "        [ 4.23305959e-04,  5.48109645e-04,  3.96324962e-04, ...,\n",
       "         -4.84902615e-04,  1.17928263e-04, -2.15656182e-04],\n",
       "        ...,\n",
       "        [-8.98150669e-04,  8.15560576e-04,  1.25550234e-03, ...,\n",
       "          2.67189957e-04,  7.58058624e-04,  6.30756665e-04],\n",
       "        [-1.07362994e-03,  8.86726193e-04,  1.06604176e-03, ...,\n",
       "          3.15071899e-04,  1.09415886e-03,  6.49177877e-04],\n",
       "        [-1.29209401e-03,  8.81772430e-04,  8.29992001e-04, ...,\n",
       "          1.69372986e-04,  1.35422742e-03,  6.31080067e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-workshop",
   "metadata": {},
   "source": [
    "#### dataset.take(1)를 통해서 1개의 배치, 즉 300개의 문장 데이터를 가져와 봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "labeled-organizer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-extreme",
   "metadata": {},
   "source": [
    "#### 이제 모델이 학습할 준비가 완료되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relative-holly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "585/585 [==============================] - 90s 153ms/step - loss: 3.6748\n",
      "Epoch 2/12\n",
      "585/585 [==============================] - 90s 153ms/step - loss: 3.2074\n",
      "Epoch 3/12\n",
      "585/585 [==============================] - 93s 159ms/step - loss: 3.0261\n",
      "Epoch 4/12\n",
      "585/585 [==============================] - 95s 163ms/step - loss: 2.8951\n",
      "Epoch 5/12\n",
      "585/585 [==============================] - 97s 165ms/step - loss: 2.7868\n",
      "Epoch 6/12\n",
      "585/585 [==============================] - 94s 160ms/step - loss: 2.6902\n",
      "Epoch 7/12\n",
      "585/585 [==============================] - 94s 160ms/step - loss: 2.6010\n",
      "Epoch 8/12\n",
      "585/585 [==============================] - 93s 160ms/step - loss: 2.5172\n",
      "Epoch 9/12\n",
      "585/585 [==============================] - 93s 159ms/step - loss: 2.4380\n",
      "Epoch 10/12\n",
      "585/585 [==============================] - 93s 159ms/step - loss: 2.3632\n",
      "Epoch 11/12\n",
      "585/585 [==============================] - 90s 154ms/step - loss: 2.2927\n",
      "Epoch 12/12\n",
      "585/585 [==============================] - 93s 159ms/step - loss: 2.2253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd32333e790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-cradle",
   "metadata": {},
   "source": [
    "#### 에포크를 10으로 주고 모델을 학습시켜 봤더니 로스값이 2.3414가 나왔네요. 에포크를 좀더 늘려보면 로스값이 더 떨어질것 같아서 2만 더 늘려 보았습니다. 예상대로 로스값이 더 떨어져서 2.2253이 나왔네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "disabled-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-traveler",
   "metadata": {},
   "source": [
    "#### 이제 모델을 평가해 보겠습니다. 얼마나 근사한 문장을 만들어 낼지 궁금하네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "metric-communication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you , i love you <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-discount",
   "metadata": {},
   "source": [
    "#### 별거 아닌 문장 같은데 심오합니다 ㅎㅎㅎ. 이렇게 스스로 문장을 만들어 낼수 있다는게 정말 신기하네요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-power",
   "metadata": {},
   "source": [
    "### 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-entry",
   "metadata": {},
   "source": [
    "#### 이번 주제는 관심이 있는 분야여서 그런지 재미있었습니다. 그런데 재미와는 별개로 어렵기도 하더라구요. 아직도 코드에 대한 해석력이나 어느 부분에서 어떤 코드를 입력 해야 하는지 잘 구분이 되지 않아 구글링을 진짜 열나게 많이 했더랬습니다. 그래도 이렇게 한문장을 출력하고 나니 힘든게 조금은 보상받는 느낌입니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
