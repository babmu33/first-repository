{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image = sorted(glob('./data/train/input/img/*.png'))\n",
    "list_label = sorted(glob('./data/train/label/mask/*.png'))\n",
    "print (len(list_image), len(list_label))\n",
    "\n",
    "IMAGE_SHAPE = (80, 120)\n",
    "data_root = './data/train/input'\n",
    "label_root = './data/train/label'\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data = image_generator.flow_from_directory(str(data_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "label_data = label_generator.flow_from_directory(str(label_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_generation(train_generator, label_generator):\n",
    "    h, w = train_generator.target_size\n",
    "    for images, labels in zip(train_generator, label_generator):\n",
    "        images /= 255.\n",
    "        images = images[..., ::-1] # rgb to bgr\n",
    "\n",
    "        list_point_labels = []\n",
    "        for img, label in zip(images, labels):\n",
    "\n",
    "            eye_ls = np.where(label==1) # leftside\n",
    "            eye_rs = np.where(label==2) # rightside\n",
    "            eye_center = np.where(label==3)\n",
    "\n",
    "            lx, ly = [eye_ls[1].mean(), eye_ls[0].mean()]\n",
    "            rx, ry = [eye_rs[1].mean(), eye_rs[0].mean()]\n",
    "            cx, cy = [eye_center[1].mean(), eye_center[0].mean()]\n",
    "\n",
    "            if len(eye_ls[0])==0 or len(eye_ls[1])==0:\n",
    "                lx, ly = [0, 0]\n",
    "            if len(eye_rs[0])==0 or len(eye_rs[1])==0:\n",
    "                rx, ry = [w, h]\n",
    "            if len(eye_center[0])==0 or len(eye_center[1])==0:\n",
    "                cx, cy = [0, 0]\n",
    "\n",
    "            np_point_label = np.array([lx/w,ly/h,rx/w,ry/h,cx/w,cy/h], dtype=np.float32)\n",
    "\n",
    "            list_point_labels.append(np_point_label)\n",
    "        np_point_labels = np.array(list_point_labels)\n",
    "        yield (images, np_point_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train_generator = user_generation(image_data, label_data)\n",
    "for i in range(2):\n",
    "    dd = next(user_train_generator)\n",
    "    print (dd[0][0].shape, dd[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tf hub feature_extractor '''\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                            input_shape=(80,120,3))\n",
    "\n",
    "image_batch = next(image_data)\n",
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "feature_extractor_layer.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_step_decay(epoch):\n",
    "    init_lr = 0.0005 #self.flag.initial_learning_rate\n",
    "    lr_decay = 0.5 #self.flag.learning_rate_decay_factor\n",
    "    epoch_per_decay = 2 #self.flag.epoch_per_decay\n",
    "    lrate = init_lr * math.pow(lr_decay, math.floor((1+epoch)/epoch_per_decay))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = image_data.samples//image_data.batch_size\n",
    "print (image_data.samples, image_data.batch_size, steps_per_epoch)\n",
    "# 20160 32 630 -> 데이터를 batch_size 의 배수로 준비해 주세요.\n",
    "\n",
    "learning_rate = LearningRateScheduler(lr_step_decay)\n",
    "\n",
    "history = model.fit(user_train_generator, epochs=20,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks = [learning_rate]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(13, 5))\n",
    "axs[0].plot(history.history['loss'], 'r')\n",
    "axs[0].set_title('loss function')\n",
    "\n",
    "axs[1].plot(history.history['mae'], 'g')\n",
    "axs[1].set_title('mae function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (80, 120)\n",
    "val_data_root = './data/val/input'\n",
    "val_label_root = './data/val/label'\n",
    "\n",
    "image_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data_val = image_generator.flow_from_directory(str(val_data_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)\n",
    "label_data_val = label_generator.flow_from_directory(str(val_label_root), class_mode=None, target_size=IMAGE_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_val_generator = user_generation(image_data_val, label_data_val)\n",
    "mse, mae = model.evaluate_generator(user_val_generator, image_data_val.n // 32)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img test\n",
    "# img = cv2.imread('./data/val/input/img/eye_000010_l.png')\n",
    "img = cv2.imread('./data/eye.png')\n",
    "\n",
    "img = cv2.resize(img, (60, 40))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_inputs = np.expand_dims(cv2.resize(img, (120, 80)), axis=0)\n",
    "preds = model.predict(np_inputs/255., 1)\n",
    "\n",
    "repred = preds.reshape((1, 3, 2))\n",
    "repred[:,:,0] *= 120\n",
    "repred[:,:,1] *= 80\n",
    "print (repred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "show = img.copy()\n",
    "for pt in repred[0]:\n",
    "    show = cv2.circle(show, tuple((pt*0.6).astype(int)), 3, (0,255,255), -1)\n",
    "    print (pt.round()*0.5)\n",
    "    \n",
    "plt.imshow(cv2.cvtColor(show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = cv2.imread('./data/test1.png')\n",
    "cropped_img = right[170: 1000, 300: 1000]\n",
    "plt.imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(repred[0][2][0].round()*0.6)\n",
    "y = int(repred[0][2][1].round()*0.6)\n",
    "w = 30\n",
    "h = 30\n",
    "print ('(x,y) : (%d,%d)'%(x,y))\n",
    "print ('(w,h) : (%d,%d)'%(w,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sticker = cv2.resize(cropped_img, (w,h))\n",
    "print (img_sticker.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_x = x - w // 2  # left\n",
    "refined_y = y - h // 2      # top\n",
    "print ('(x,y) : (%d,%d)'%(refined_x, refined_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sticker = img_sticker[refined_y:]\n",
    "print (img_sticker.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sticker_area = show[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "img[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "    np.where(img_sticker==255,sticker_area,img_sticker).astype(np.uint8)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-mason",
   "metadata": {},
   "source": [
    "눈동자의 중심은 잘 찾고 있지만, 눈의 양쪽은 잘 못찾는 문제가 있습니다.  \n",
    "데이터를 더 추가하거나 annotation 도구를 통해서 좋은 데이터로 학습을 한다면 더 좋은 결과가 나올 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-banking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
