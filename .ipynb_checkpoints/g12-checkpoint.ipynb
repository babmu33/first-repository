{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "manufactured-gibson",
   "metadata": {},
   "source": [
    "#### 진행 순서\n",
    "- Recognition model 구현\n",
    "- 해당 이미지 내의 문자 찾아내기\n",
    "- 단어의 영역을 잘라오기\n",
    "- 시각화\n",
    "- Recognition model\n",
    "- 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras_ocr\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
    "\n",
    "TRAIN_DATA_PATH = HOME_DIR+'/MJ/MJ_train'\n",
    "VALID_DATA_PATH = HOME_DIR+'/MJ/MJ_valid'\n",
    "TEST_DATA_PATH = HOME_DIR+'/MJ/MJ_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getenv('HOME'),'aiffel/ocr/')\n",
    "SAMPLE_IMG_PATH = path+'sample.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "env = lmdb.open(TRAIN_DATA_PATH, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32 )\n",
    "        \n",
    "        print('target_img_size:{}'.format(target_img_size))\n",
    "        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MJDatasetSequence(Sequence):\n",
    "    def __init__(self, \n",
    "                      dataset_path,\n",
    "                      label_converter,\n",
    "                      batch_size=1,\n",
    "                      img_size=(100,32),\n",
    "                      max_text_len=22,\n",
    "                      is_train=False,\n",
    "                      character=''\n",
    "                ):\n",
    "        \n",
    "        self.label_converter = label_converter\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.character = character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.num_samples = int(num_samples)\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    def _get_img_label(self, index):\n",
    "        # Return image array and label in string\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width,self.img_size[1] )\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            label = label.upper()[:self.max_text_len]\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64')*self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "        return inputs, outputs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.index_list =  [index + 1 for index in range(self.num_samples)]\n",
    "        if self.is_train :\n",
    "            np.random.shuffle(self.index_list)\n",
    "            return self.index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelConverter(object):\n",
    "     \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "     def __init__(self, character):\n",
    "         self.character = \"-\" + character\n",
    "         self.label_map = dict()\n",
    "         for i, char in enumerate(self.character):\n",
    "             self.label_map[char] = i\n",
    "\n",
    "     def encode(self, text):\n",
    "         encoded_label = []\n",
    "         for i, char in enumerate(text):\n",
    "             if i > 0 and char == text[i - 1]:\n",
    "                 encoded_label.append(0)\n",
    "             encoded_label.append(self.label_map[char])\n",
    "         return np.array(encoded_label)\n",
    "\n",
    "     def decode(self, encoded_label):\n",
    "         target_characters = list(self.character)\n",
    "         decoded_label = \"\"\n",
    "         for encode in encoded_label:\n",
    "             decoded_label += self.character[encode]\n",
    "         return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "\n",
    "    # Build CRNN model\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(\n",
    "        inputs=model_input,\n",
    "        outputs=loss_out\n",
    "    )\n",
    "    y_func = tf.keras.backend.function(image_input, [y_pred])\n",
    "    return model, y_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "sgd = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
    ")\n",
    "model.fit(train_set,\n",
    "        steps_per_epoch=len(val_set),\n",
    "        epochs=100,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=len(val_set),\n",
    "        callbacks=[ckp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img_path):\n",
    "    pipeline = keras_ocr.pipeline.Pipeline()\n",
    "    \n",
    "    images = keras_ocr.tools.read(img_path)\n",
    "    predictions = pipeline.recognize([img_path])\n",
    "    \n",
    "    result_img = keras_ocr.tools.drawAnnotations(image=images, predictions=predictions[0])\n",
    "\n",
    "    cropped_imgs = []\n",
    "    revised_boxes = []\n",
    "    for _, box in predictions[0]:\n",
    "        revised_boxes.append(box)\n",
    "    boxes = revised_boxes\n",
    "\n",
    "    crop_box =[]\n",
    "    for box in boxes:\n",
    "        cropped_imgs.append(keras_ocr.tools.warpBox(image=images, box=box))\n",
    "\n",
    "\n",
    "    return result_img, cropped_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil, cropped_img = detect_text(SAMPLE_IMG_PATH)\n",
    "display(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(cropped_img[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(cropped_img[1])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_img(pil_img, input_img_size=(100,32)):\n",
    "    img90 = cv2.rotate(pil_img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    img_flip = cv2.flip(img90, 0)\n",
    "    pil_img = cv2.resize(img_flip, (32, 100), interpolation = cv2.INTER_CUBIC)\n",
    "    img = pil_img.reshape(1, 100, 32, 3)\n",
    "    output = model_pred.predict(img)\n",
    "    result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
    "    print(\"Result: \\t\", result)\n",
    "    display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _img in cropped_img:\n",
    "    recognize_img(_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-democracy",
   "metadata": {},
   "source": [
    "추론 결과 아래 이미지니 SLEEPER은 제대로 학습이 됐습니다.  \n",
    "하지만 위의 그림은 우리가 학습시킨 데이터로는 잘 번역이 안되는 것 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-minnesota",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-madagascar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-manner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-boating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-power",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-bible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-continuity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
