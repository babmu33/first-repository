{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liberal-cedar",
   "metadata": {},
   "source": [
    "# 7. 프로젝트: Spectrogram classification 모델 구현   \n",
    "\n",
    "오늘 실습에서 1차원 Waveform 데이터를 입력받아 Text 라벨을 출력하는 모델을 기본 버전과 Skip-connection 버전으로 나누어 학습시켜 보았습니다. 이번에는 2차원 Spectrogram 데이터를 입력받아 위 모델과 동일한 역할을 수행하는 모델을 아래 제시된 단계와 같이 수행해 보시기 바랍니다. 이번에도 마찬가지로 기본 버전과 Skip-connection 버전으로 나누어 각각 진행해 보시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-hormone",
   "metadata": {},
   "source": [
    "## 1. 데이터 처리와 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-albert",
   "metadata": {},
   "source": [
    "### 라벨 데이터 처리하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advisory-square",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = os.getenv(\"HOME\")+'/aiffel/speech_recognition/data/speech_wav_8000.npz'\n",
    "speech_data = np.load(data_path)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-ranch",
   "metadata": {},
   "source": [
    "#### 데이터를 살펴보니 1초 길이의 오디오 음성데이터 50620개로 이루어져 있고, 16000개의 sample rate를 8000개로 re-sampling해서 사용하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordinary-dispute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL :  ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
      "Indexed LABEL :  {'yes': 0, 'no': 1, 'up': 2, 'down': 3, 'left': 4, 'right': 5, 'on': 6, 'off': 7, 'stop': 8, 'go': 9, 'unknown': 10, 'silence': 11}\n"
     ]
    }
   ],
   "source": [
    "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "\n",
    "label_value = target_list\n",
    "label_value.append('unknown')\n",
    "label_value.append('silence')\n",
    "\n",
    "print('LABEL : ', label_value)\n",
    "\n",
    "new_label_value = dict()\n",
    "for i, l in enumerate(label_value):\n",
    "    new_label_value[l] = i\n",
    "label_value = new_label_value\n",
    "\n",
    "print('Indexed LABEL : ', new_label_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-tract",
   "metadata": {},
   "source": [
    "#### 학습을 위해서 현재 Text 형태의 파일을 index 형태로 바꿔주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "running-motivation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  3, ..., 11, 11, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for v in speech_data[\"label_vals\"]:\n",
    "    temp.append(label_value[v[0]])\n",
    "label_data = np.array(temp)\n",
    "\n",
    "label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-praise",
   "metadata": {},
   "source": [
    "#### 살펴보니 다 int로 이루진 데이터가 잘 배열되어 있는 걸  확인할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automotive-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def wav2spec(wav, fft_size=258): # spectrogram shape을 맞추기위해서 size 변형\n",
    "    D = np.abs(librosa.stft(wav, n_fft=fft_size))\n",
    "    return D\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-buddy",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 위의 1차원 데이터는 다루어 봤으니 바로 2차원 데이터로 학습을 시키는 과제를 해볼까요? 일단 FFT 관련 라이브러리인 librosa를 설치하고 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "described-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = []\n",
    "for wav in speech_data[\"wav_vals\"]:\n",
    "    spec.append(wav2spec(wav))\n",
    "spec = np.array(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sorted-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave data shape :  (50620, 8000)\n",
      "Label data shape :  (50620, 130, 126)\n"
     ]
    }
   ],
   "source": [
    "print(\"Wave data shape : \", speech_data[\"wav_vals\"].shape)\n",
    "print(\"Label data shape : \", spec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-beatles",
   "metadata": {},
   "source": [
    "#### 1차원의 Waveform 데이터가 2차원의 Spectrogram 데이터로 변환된것을 확인 할수가 있습니다. 이부분이 너무 어려웠습니다. 의외로 간단한 함수였는데 이걸 해결하지 못해 반나절을 그냥 보낸것 같아요. 아... 내시간... 그 시간에 밀린 코딩 숙제를 했어야 했는데... TT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-barrier",
   "metadata": {},
   "source": [
    "### sklearn의 train_test_split함수를 이용하여 train, test 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sr = 8000\n",
    "train_spec, test_spec, train_label, test_label = train_test_split(spec, \n",
    "                                                                label_data, \n",
    "                                                                test_size=0.1,\n",
    "                                                                shuffle=True)\n",
    "print(train_spec.shape)\n",
    "print(train_label.shape)\n",
    "print(test_spec.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-magnitude",
   "metadata": {},
   "source": [
    "#### split함수를 이용하여 train, test 분리 했습니다. 45558개의 train set과 5062개의 test set으로 분리가 되었네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "del speech_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "del spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-camping",
   "metadata": {},
   "source": [
    "#### 메모리 용량이 딸리는지 계속 작동이 멈춰서 너무 힘들었습니다. 이것 저것 다 해봐도 이유를 알수가 없어서 재부팅을 했더니 겨우 여기까지는 넘어왔습니다. 그리고 불필요한 데이터를 지웠는데 자꾸 멈출까봐 조마조마 하네요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-cargo",
   "metadata": {},
   "source": [
    "## 2. 학습을 위한 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "\n",
    "# the save point\n",
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/speech_recognition/models/wav'\n",
    "\n",
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-heritage",
   "metadata": {},
   "source": [
    "#### 겁나서 에포크를 많이 줄수가 없네요. 맘같아선 10을 주고 싶지만 밤을 새야 할것 같은 불길한 예감이 들어 5로줬습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-march",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-interview",
   "metadata": {},
   "source": [
    "### tf.data.Dataset을 이용 \n",
    "### from_tensor_slices 함수에 return 받길 원하는 데이터를 튜플 (data, label) 형태로 넣어서 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_label(spec, label):\n",
    "    label = tf.one_hot(label, depth=12)\n",
    "    return spec, label\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-bahrain",
   "metadata": {},
   "source": [
    "### map과 batch를 이용한 데이터 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_spec, train_label))\n",
    "train_dataset = train_dataset.map(one_hot_label)\n",
    "train_dataset = train_dataset.repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_spec, test_label))\n",
    "test_dataset = test_dataset.map(one_hot_label)\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-driver",
   "metadata": {},
   "source": [
    "#### tf.data.Dataset 함수를 이용해 튜플 형태의 데이터를 얼마나 가져올지 결정했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-ceiling",
   "metadata": {},
   "source": [
    "## 4. 2차원 Spectrogram 데이터를 처리하는 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-galaxy",
   "metadata": {},
   "source": [
    "### 2차원 Spectrogram 데이터의 시간축 방향으로 Conv1D layer를 적용\n",
    "### batchnorm, dropout, dense layer 등을 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "input_tensor = layers.Input(shape=(130, 126, 1))\n",
    "\n",
    "x = layers.Conv2D(32, 9, padding='same', activation='relu')(input_tensor)\n",
    "x = layers.Conv2D(32, 9, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(64, 9, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 9, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(128, 9, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, 9, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(128, 9, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 9, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, 9, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(256, 9, padding='same', activation='relu')(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "output_tensor = layers.Dense(12)(x)\n",
    "\n",
    "model_wav = tf.keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "model_wav.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-wells",
   "metadata": {},
   "source": [
    "####  데이터가 2차원의 Spectrogram 데이터이기 때문에 그에맞는 shape값을 주었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-private",
   "metadata": {},
   "source": [
    "\n",
    "### 12개의 단어 class를 구분하는 loss를 사용하고 Adam optimizer를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    "model_wav.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-break",
   "metadata": {},
   "source": [
    "### 모델 가중치를 저장하는 checkpoint callback 함수 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-charge",
   "metadata": {},
   "source": [
    "### 다양한 모델의 실험을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30분 내외 소요 (메모리 사용량에 주의해 주세요.)\n",
    "history_wav = model_wav.fit(train_dataset, epochs=max_epochs,\n",
    "                    steps_per_epoch=len(train_spec) // batch_size,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(test_spec) // batch_size,\n",
    "                    callbacks=[cp_callback]\n",
    "                    )\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-performer",
   "metadata": {},
   "source": [
    "#### tensorflower gpu도 잘 설치 되어있고 작동 되는것도 확인이 되었는데 너무 시간이 오래 걸립니다 ㅜㅜ 한 에포크당 10분이 소요 되는 바람에 너무 오래 걸려 다음 진행이 어려웠습니다. 이거부터 해결해야 다음 진행도 될듯한데... 산넘어 산이네 ... 잠은 언제 잔다냐!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-absorption",
   "metadata": {},
   "source": [
    "## 5. 학습 후, 학습이 어떻게 진행됐는지 그래프로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-australia",
   "metadata": {},
   "source": [
    "### loss, accuracy를 그래프로 표현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_wav.history['accuracy']\n",
    "val_acc = history_wav.history['val_accuracy']\n",
    "\n",
    "loss=history_wav.history['loss']\n",
    "val_loss=history_wav.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-melissa",
   "metadata": {},
   "source": [
    "#### 데이터 양이 충분해서 인지 accuracy는 높고, loss값은 낮은걸로 봐서 학습이 아주 잘된듯 보입니다. 뿌듯... 내가한건 아니지만 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-detective",
   "metadata": {},
   "source": [
    "## 6. Test dataset을 이용해서 모델의 성능을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-argument",
   "metadata": {},
   "source": [
    "### 저장한 weight 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wav.load_weights(checkpoint_dir)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-section",
   "metadata": {},
   "source": [
    "### 모델의 예측값과 정답값이 얼마나 일치하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_wav.evaluate(test_dataset)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(results[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.4f}%\".format(results[1]*100))\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-accident",
   "metadata": {},
   "source": [
    "#### 이렇게 수치로 환산하고 보니 정말 학습이 잘 되었군요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_label_value = {v: k for k, v in label_value.items()}\n",
    "batch_index = np.random.choice(len(test_spec), size=1, replace=False)\n",
    "\n",
    "batch_xs = test_spec[batch_index]\n",
    "batch_ys = test_label[batch_index]\n",
    "y_pred_ = model_wav(batch_xs, training=False)\n",
    "a= librosa.istft(batch_xs.reshape(130,126))\n",
    "\n",
    "print(\"label : \", str(inv_label_value[batch_ys[0]]))\n",
    "\n",
    "ipd.Audio(a, rate=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-elder",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.argmax(y_pred_) == batch_ys[0]:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Correct!)')\n",
    "else:\n",
    "    print(\"y_pred: \" + str(inv_label_value[np.argmax(y_pred_)]) + '(Incorrect!)')\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-baptist",
   "metadata": {},
   "source": [
    "#### 와우~ 정말 완벽하게 맞췄네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(batch_xs.reshape(130,126), ref=np.max), x_axis='time')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.xticks(range(0, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-planet",
   "metadata": {},
   "source": [
    "#### 스펙트로그램이 정말 예쁘게 잘 그려졌네요. 앗싸~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-judge",
   "metadata": {},
   "source": [
    "## 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-bridal",
   "metadata": {},
   "source": [
    "### 주말을 이 과제로 다 날렸지만, 그래도 하나씩 해결되는 것들을 보니 눈물이 납니다.  컴맹이던 내가 지금 이걸 하고 있다니...TT 아멘!! 데이터가 너무 커서 리부팅을 20번도 넘게 하는 바람에 뚜껑이 열려 한겨울도 한여름처럼 느껴졌어요. 그래도 해냈으니 다행입니다.   아직 모든것들이 다 이해되지는 않지만 팀들의 도움을 받아서 조금씩 성장하는 즐거움을 느끼고 있네요. 텍스트 데이터부터 이미지 데이터를 지나 이제 음성데이터까지 하나씩 알아가게 되서 힘들지만 보람됩니다. 이제 자러갑니다. zzzzzzz~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
