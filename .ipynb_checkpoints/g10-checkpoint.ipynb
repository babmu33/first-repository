{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cars196',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "base_model = keras.applications.resnet.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224,3),\n",
    "    pooling='avg',\n",
    ")\n",
    "x = base_model.output\n",
    "preds = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "cam_model=keras.Model(inputs=base_model.input, outputs=preds)\n",
    "cam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(input):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_norm = apply_normalize_on_dataset(ds_train)\n",
    "ds_test_norm = apply_normalize_on_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2020)\n",
    "cam_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cam_model = cam_model.fit(\n",
    "    ds_train_norm,\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=15,\n",
    "    validation_data=ds_test_norm,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/class_activation_map/cam_model.h5'\n",
    "cam_model.save(cam_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(ds):\n",
    "    ds = ds.take(1)\n",
    "    sample_data = list(ds.as_numpy_iterator())\n",
    "    bbox = sample_data[0]['bbox']\n",
    "    image = sample_data[0]['image']\n",
    "    label = sample_data[0]['label']\n",
    "    return sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model = tf.keras.models.load_model(cam_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    \n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # 학습한 모델에서 원하는 Layer의 output을 얻기 위해서 모델의 input과 output을 새롭게 정의해줍니다.\n",
    "    # model.layers[-3].output에서는 우리가 필요로 하는 GAP 이전 Convolution layer의 output을 얻을 수 있습니다.\n",
    "    cam_model = tf.keras.models.Model([model.inputs], [model.layers[-3].output, model.output])\n",
    "    conv_outputs, predictions = cam_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "    class_weights = model.layers[-1].get_weights()[0] #마지막 모델의 weight activation을 가져옵니다.\n",
    "    \n",
    "    cam_image = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, class_idx]):\n",
    "        # W * f 를 통해 class별 activation map을 계산합니다.\n",
    "        cam_image += w * conv_outputs[:, :, i]\n",
    "\n",
    "    cam_image /= np.max(cam_image) # activation score를 normalize합니다.\n",
    "    cam_image = cam_image.numpy()\n",
    "    cam_image = cv2.resize(cam_image, (width, height)) # 원래 이미지의 크기로 resize합니다.\n",
    "    return cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam_on_image(src1, src2, alpha=0.5):\n",
    "    beta = (1.0 - alpha)\n",
    "    merged_image = cv2.addWeighted(src1, alpha, src2, beta, 0.0)\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_image = item['image'].astype(np.uint8)\n",
    "cam_image_3channel = np.stack([cam_image*255]*3, axis=-1).astype(np.uint8)\n",
    "\n",
    "blended_image = visualize_cam_on_image(cam_image_3channel, origin_image)\n",
    "plt.imshow(blended_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, activation_layer, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # Grad cam에서도 cam과 같이 특정 레이어의 output을 필요로 하므로 모델의 input과 output을 새롭게 정의합니다.\n",
    "    # 이때 원하는 레이어가 다를 수 있으니 해당 레이어의 이름으로 찾은 후 output으로 추가합니다.\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(activation_layer).output, model.output])\n",
    "    \n",
    "    # Gradient를 얻기 위해 tape를 사용합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, pred = grad_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "        loss = pred[:, class_idx] # 원하는 class(여기서는 정답으로 활용) 예측값을 얻습니다.\n",
    "        output = conv_output[0] # 원하는 layer의 output을 얻습니다.\n",
    "        grad_val = tape.gradient(loss, conv_output)[0] # 예측값에 따른 Layer의 gradient를 얻습니다.\n",
    "\n",
    "    weights = np.mean(grad_val, axis=(0, 1)) # gradient의 GAP으로 class별 weight를 구합니다.\n",
    "    grad_cam_image = np.zeros(dtype=np.float32, shape=conv_output.shape[0:2])\n",
    "    for k, w in enumerate(weights):\n",
    "        # 각 class별 weight와 해당 layer의 output을 곱해 class activation map을 얻습니다.\n",
    "        grad_cam_image += w * output[:, :, k]\n",
    "        \n",
    "    grad_cam_image /= np.max(grad_cam_image)\n",
    "    grad_cam_image = grad_cam_image.numpy()\n",
    "    grad_cam_image = cv2.resize(grad_cam_image, (width, height))\n",
    "    return grad_cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv5_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv4_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv3_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(cam_image, score_thresh=0.05):\n",
    "    low_indicies = cam_image <= score_thresh\n",
    "    cam_image[low_indicies] = 0\n",
    "    cam_image = (cam_image*255).astype(np.uint8)\n",
    "    \n",
    "    contours,_ = cv2.findContours(cam_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    rotated_rect = cv2.minAreaRect(cnt)\n",
    "    rect = cv2.boxPoints(rotated_rect)\n",
    "    rect = np.int0(rect)\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect의 좌표는 (x, y) 형태로, bbox는 (y_min, x_min, y_max, x_max)의 normalized 형태로 주어집니다. \n",
    "def rect_to_minmax(rect, image):\n",
    "    bbox = [\n",
    "        rect[:,1].min()/float(image.shape[0]),  #bounding box의 y_min\n",
    "        rect[:,0].min()/float(image.shape[1]),  #bounding box의 x_min\n",
    "        rect[:,1].max()/float(image.shape[0]), #bounding box의 y_max\n",
    "        rect[:,0].max()/float(image.shape[1]) #bounding box의 x_max\n",
    "    ]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect, item['image'])\n",
    "pred_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxA, boxB):\n",
    "    y_min = max(boxA[0], boxB[0])\n",
    "    x_min= max(boxA[1], boxB[1])\n",
    "    y_max = min(boxA[2], boxB[2])\n",
    "    x_max = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iou(pred_bbox, item['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = copy.deepcopy(item['image'])\n",
    "rect = get_bbox(cam_image)\n",
    "\n",
    "image = cv2.drawContours(image,[rect],0,(0,0,255),2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect, item['image'])\n",
    "get_iou(pred_bbox, item['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv5_block2_out', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = copy.deepcopy(item['image'])\n",
    "rect_g = get_bbox(grad_cam_image)\n",
    "\n",
    "image = cv2.drawContours(image,[rect_g],0,(0,0,255),2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect_g, item['image'])\n",
    "get_iou(pred_bbox, item['bbox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-queensland",
   "metadata": {},
   "source": [
    "#### 결과\n",
    "위의 그림에서는 'conv5_block3_out' 보다 'conv5_block2_out'를 사용할 때 더 성능이 잘 나왔습니다.  \n",
    "Grad-CAM보다 CAM의 성능이 좋을 때도 많았고, Grad-Cam에서 몇 번째 레이어를 추출하느냐에 따라 성능 차이가 있었습니다.  \n",
    "CAM의 성능이 좋지 않을 때 Grad-CAM을 사용해서 성능을 개선시킨다면 좋은 결과를 얻을 수 있을 것입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
