{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-recovery",
   "metadata": {},
   "source": [
    "## 7-1. 들어가며\n",
    "\n",
    "[(오른쪽) 인물사진 모드]\n",
    "https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html\n",
    "\n",
    "우리가 그동안 다뤄온 이미지 분류(image classification), 객체 인식(object detection) 모델은 이미지에서 어떤 물체의 종류를 분류하거나 물체의 존재와 위치를 탐지해 냅니다.\n",
    "이번에 배워볼 세그멘테이션(segmentation)은 픽셀 수준에서 이미지의 각 부분이 어떤 의미를 갖는 영역인지 분리해 내는 방법입니다.\n",
    "\n",
    "위 그림을 보면 세그멘테이션 기술을 쉽게 이해할 수 있습니다. 세그멘테이션은 이미지 분할 기술입니다. 위 사진에서 세그멘테이션 모델은 사람의 영역과 배경영역을 분리해서 배경을 흐리게 처리하여 아웃포커싱 효과를 보여줍니다.\n",
    "\n",
    "세그멘테이션은 이미지의 영역을 어떤 영역인지 분리해내는 기술로 다양한 영역에서 활용되고 있습니다. 위 사진은 실제로 구글의 스마트폰인 Pixel 2에서 쓰이는 인물사진 모드(portrait mode)라고 합니다. 아마도 우리의 핸드폰 속에는 이렇게 알게 모르게 다양한 딥러닝 기반의 기술이 숨어있을 것입니다. 그럼 이제 세그멘테이션을 하나씩 파헤쳐 보겠습니다.\n",
    "\n",
    "### 실습 목표\n",
    "\n",
    "    1.세그멘테이션의 방식을 공부합니다.\n",
    "    2.시맨틱 세그멘테이션 모델의 결괏값을 이해합니다.\n",
    "    3.시맨틱 세그멘테이션을 위한 접근 방식을 이해합니다.\n",
    "\n",
    "### 학습 내용\n",
    "\n",
    "    1.세그멘테이션 문제의 종류\n",
    "    2.주요 세그멘테이션 모델\n",
    "        FCN\n",
    "        U-NET\n",
    "        DeepLab 계열\n",
    "    3.세그멘테이션의 평가\n",
    "    4.Upsampling의 다양한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-providence",
   "metadata": {},
   "source": [
    "## 7-2. 세그멘테이션 문제의 종류\n",
    "\n",
    "세그멘테이션 모델들은 다양한 분야에서 활용되고 있습니다. 자율주행 카메라에 적용되어 이미지의 의미를 해석하기도 하고 의료 이미지에서 병이 있는 영역을 판별해내기도 합니다.\n",
    "\n",
    "이때 이미지 내에서 영역을 분리하는 접근 방식은 크게 두 가지 방식이 있습니다. 바로 시맨틱 세그멘테이션(semantic segmentation)과 인스턴스 세그멘테이션(instance segmentation)입니다. 영역을 분리한다는 관점에서 비슷하지만, 접근 방식에 따라 문제의 정의와 모델을 구성하는 방식이 달라집니다. 아래 그림은 그러한 좋은 예시를 보여 줍니다.\n",
    "\n",
    "http://machinelearningkorea.com/2019/07/13/%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%B3%84-%EB%B6%84%EB%A5%98%EB%AC%B8%EC%A0%9C/\n",
    "\n",
    "먼저 위 그림의 상단 두 사진을 볼까요? 좌상단 사진은 양들이 길에 있는 사진에서 단지 어떤 물체들이 모여 있는 영역의 위치를 인식(localization)과 이 물체들이 양이라는 것을 판별(classification)하는 접근법을 보여줍니다. 그러나 우상단 사진은 비슷하지만 개별 양들의 개체 하나하나의 위치를 정확히 식별하는 객체 인식(object detection)의 접근법을 보여 줍니다.\n",
    "\n",
    "하단의 두 사진도 이와 비슷한 대비를 보여줍니다. 두 사진 모두 세그멘테이션의 결과를 보여주는 사진이라는 점에서 동일합니다. 좌하단의 경우 양, 길, 풀밭의 영역을 정확히 구분해낼 뿐 양들 각각의 구분해 내진 않습니다. 이런 접근 방식을 시맨틱 세그멘테이션이라고 합니다. 그러나 우하단의 경우 각 양의 개체들의 영역을 픽셀 단위로 정확히 구분해 내고 있습니다. 이것은 인스턴스 세그멘테이션의 사례가 됩니다.\n",
    "\n",
    "### 1) 시맨틱 세그멘테이션(Semantic Segmentation)\n",
    "\n",
    "https://mi.eng.cam.ac.uk/projects/segnet/#demo 캡처\n",
    "\n",
    "https://www.researchgate.net/figure/The-example-cucumber-leaf-images-a-scanned-image-before-segmentation-with-necrotic_fig2_273959785\n",
    "\n",
    "위 사진들은 모두 시맨틱 세그멘테이션의 또다른 사례들을 보여 줍니다.\n",
    "\n",
    "아래 SegNet 데모를 통해서도 시맨틱 세그멘테이션을 직관적으로 이해하는 데 도움을 얻을 수 있을 것입니다. 직접 시맨틱 세그멘테이션을 사용해 본 후, 다음 퀴즈에 답변해 봅시다.\n",
    "\n",
    "SegNet 데모 사이트 ttps://mi.eng.cam.ac.uk/projects/segnet/#demo\n",
    "\n",
    "https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "위 모델 구조는 시맨틱 세그멘테이션의 대표적인 모델인 U-Net의 구조입니다. 다음 스텝에서 좀더 자세하게 다룰 예정입니다. 간단히 구조만 살펴보자면, 입력으로 572x572 크기인 이미지가 들어가고 출력으로 388x388의 크기에 두 가지의 클래스를 가진 세그멘테이션 맵(segmentation map)이 나옵니다. 두 가지 클래스는 가장 마지막 레이어의 채널 개수가 \"2\"라는 점에서 확인할 수 있습니다.\n",
    "\n",
    "이때 두 가지의 클래스를 문제에 따라 다르게 정의하면 클래스에 따른 시맨틱 세그멘테이션 맵(semantic segmentation map)을 얻을 수 있습니다. 처음 봤던 예제의 인물사진 모드라면 사람의 영역과 배경 클래스를, 의료 인공지능에서는 세포 사진에서 병이 있는 영역과 정상인 영역 등을 지정해 적용할 수 있습니다.\n",
    "\n",
    "이전에 우리가 봤던 이미지 분류나 물체 검출 문제보다 확실히 큰 출력값을 볼 수 있는데요. 그 이유는 세그멘테이션을 위해서 이미지의 각 픽셀에 해당하는 영역의 클래스별 정보가 필요하기 때문입니다.\n",
    "\n",
    "### 2) 인스턴스 세그멘테이션(Instance Segmentation)\n",
    "\n",
    "인스턴스 세그멘테이션은 같은 클래스 내에서도 각 개체(instance)들을 분리하여 세그멘테이션을 수행합니다. 위에서 언급한 시맨틱 세그멘테이션보다 더욱 어려운 과제일 텐데 어떻게 접근하면 좋을까요? 우리가 지금까지 배워왔던 방식으로 짐작해 본다면 물체 검출(object detection) 모델로 각 개체를 구분하고 이후에 각 개체 별로 시맨틱 세그멘테이션을 수행하면 인스턴스 세그멘테이션을 할 수 있지 않을까요?\n",
    "\n",
    "이러한 방식 중 대표적인 것이 Mask R-CNN 입니다. 2017년에 발표된 MasK-R-CNN은 2-Stage Object Detection의 가장 대표적인 Faster-R-CNN을 계승한 것으로서, Faster-R-CNN의 아이디어인 RoI(Region-of-Interest) Pooling Layer(RoIPool) 개념을 개선하여 정확한 Segmentation에 유리하게 한 RoIAlign, 그리고 클래스별 마스크 분리라는 단순한 두가지 아이디어를 통해, 클래스별 Object Detection과 시멘틱 세그멘테이션을 사실상 하나의 Task로 엮어낸 것으로 평가받는 중요한 모델입니다.\n",
    "\n",
    "여기서 잠시 Faster-R-CNN 계열에서 사용하는 RoIPool 개념과 그 한계점에 대해 잠시 알아보겠습니다.\n",
    "[RoI Pooling Layer의 개념. (https://arxiv.org/pdf/1504.08083.pdf, Fast-R-CNN 논문)]\n",
    "\n",
    "위 RoIPool Layer는 다양한 RoI 영역을 Pooling을 통해 동일한 크기의 Feature map으로 추출해 내는 레이어입니다. 이후 이 고정 사이즈의 Feature map을 바탕으로 바운딩 박스와 object의 클래스를 추론해 냅니다.\n",
    "\n",
    "이 구성은 Object Detection을 위해서는 크게 문제가 되지 않습니다. 그러나 object 영역의 정확한 마스킹을 필요로 하는 Segmentation 문제에서 문제가 되는 것은 바로 아래 그림에서 설명해 줍니다. 바로 RoIPool 과정에서 Quantization이 필요하다는 점입니다. 아래 그림의 예에서는 16X16으로 분할합니다. 이미지에서 RoI 영역의 크기는 다양한데, 모든 RoI 영역의 가로/세로 픽셀 크기가 16의 배수인 것은 아닙니다. 아래 그림의 예에서 가로 200, 세로 145픽셀짜리 RoI 영역을 16X16으로 분할된 영역 중 절반 이상이 RoI 영역에 덮히는 곳들로 끼워맞추다 보면, 어쩔 수 없이 RoI 영역 밖이 포함되는 경우도 있고, 자투리 영역이 버려지는 경우도 생기게 됩니다.\n",
    "\n",
    "이런 상황은 필연적으로 시맨틱 세그멘테이션의 정보손실과 왜곡을 야기하게 됩니다.\n",
    "\n",
    "[Understanding Region of Interest — (RoI Align and RoI Warp) [출처] https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193]\n",
    "\n",
    "Mask-R-CNN의 RoIAlign은 Quantization하지 않고도 RoI를 처리할 고정 사이즈의 Feature map을 생성할 수 있게 아이디어를 제공합니다. 아이디어의 핵심은, RoI 영역을 pooling layer의 크기에 맞추어 등분한 후, RoIPool을 했을 때의 quantization 영역 중 가까운 것들과의 bilinear interpolation 계산을 통해 생성해야 할 Feature Map을 계산해 낸다는 점입니다. 설명만으로는 정말 이해하기 어려울 것입니다. 논문 원문보다 훨씬 명확한 설명을 제공하는 아래 글을 통해 RoIAlign의 계산 방법을 다시 확인해 봅시다.\n",
    "\n",
    "(참고) Understanding Region of Interest — (RoI Align and RoI Warp) https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193\n",
    "\n",
    "Mask R-CNN은 Faster R-CNN에서 특성 추출방식을 \"RoIAlign\" 방식으로 개선을 하고 세그멘테이션을 더한 방식입니다. 아래 Faster R-CNN과 비교한 구조도를 보면 U-Net처럼 피처 맵(feature map)의 크기를 키워 마스크(mask)를 생성해내는 부분을 통해 인스턴스에 해당하는 영역, 즉 인스턴스 맵을 추론합니다. Mask R-CNN은 클래스에 따른 마스크를 예측할 때, 여러 가지 태스크를 한 모델로 학습하여 물체 검출의 성능을 높입니다.\n",
    "\n",
    "아래 그림을 보시면 Bounding box regression을 하는 Bbox head와 마스크를 예측하는 Mask Head의 두 갈래로 나뉘는 것을 볼 수 있는데요. Mask map의 경우 시맨틱 세그멘테이션과 달리 상대적으로 작은 28x28의 특성 맵 크기를 갖습니다. RoIAlign을 통해 줄어든 특성에서 마스크를 예측하기 때문에 사용하려는 목적에 따라서 정확한 마스크를 얻으려는 경우에는 부적합할 수 있습니다.\n",
    "\n",
    "https://arxiv.org/pdf/1703.06870.pdf 캡처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-profile",
   "metadata": {},
   "source": [
    "## 7-3. 주요 세그멘테이션 모델 (1) FCN\n",
    "\n",
    "이제 세그멘테이션을 위한 대표적인 방법을 몇 가지 배워보도록 하겠습니다. 많이 접해오셨을 이미지 분류나 물체 검출만큼, 세그멘테이션도 오랜 시간 그 목적에 따라서 여러 가지 모델이 발전되어 왔습니다.\n",
    "\n",
    "시간 순서대로 Fully Convolutional Networks for Semantic Segmentation 논문의 FCN(Fully Convolutional Network) 부터 보도록 하겠습니다.\n",
    "\n",
    "    주요 참고 자료\n",
    "\n",
    "Fully Convolutional Networks for Semantic Segmentation - 허다운 https://www.youtube.com/watch?v=_52dopGu3Cw\n",
    "FCN 논문 리뷰 — Fully Convolutional Networks for Semantic Segmentation https://medium.com/@msmapark2/fcn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-fully-convolutional-networks-for-semantic-segmentation-81f016d76204\n",
    "원본 논문: Fully Convolutional Networks for Semantic Segmentation https://arxiv.org/abs/1411.4038\n",
    "\n",
    "FCN은 이미지넷 챌린지(ImageNet Challenge)에서 좋은 성적을 거두었던 AlexNet, VGG-16 등의 모델을 세그멘테이션에 맞게 변형한 모델입니다. 기본적인 VGG 모델은 이미지의 특성을 추출하기 위한 네트워크의 뒷단에 fully connected layer를 붙여서 계산한 클래스별 확률을 바탕으로 이미지 분류를 수행합니다. FCN에서는 세그멘테이션을 하기 위해서 네트워크 뒷단에 fully connected layer 대신 CNN을 붙여줍니다.\n",
    "\n",
    "여기서 fully connected layer 대신 CNN을 붙여주는 이유가 뭘까요?\n",
    "\n",
    "아시다시피 CNN은 이미지 내 위치의 특성을 유지합니다. 하지만 fully connected layer는 위치를 고려하지 않게 되죠. 위치정보를 유지하면서 클래스 단위의 히트맵(heat map)을 얻어 세그멘테이션을 하기 위해, fully connected layer를 CNN으로 대체합니다.\n",
    "\n",
    "그렇다면 마지막 CNN은 어떤 세팅을 갖게 될까요? 위치의 특성을 유지하면서 이미지 분류를 하기 위해서 마지막 CNN은 1x1의 커널 크기(kernel size)와 클래스의 개수만큼의 채널을 갖습니다. 이렇게 CNN을 거치면 클래스 히트맵을 얻을 수 있습니다.\n",
    "\n",
    "하지만 히트맵의 크기는 일반적으로 원본 이미지보다 작습니다. CNN과 pooling 레이어를 거치면서 크기가 줄었기 때문인데요. 이를 키워주는 방법을 upsampling 이라고 합니다. Upsampling에는 여러 가지 방법이 있습니다. 그중 FCN에서는 Deconvolution과 Interpolation 방식을 활용합니다. Deconvolution은 컨볼루션 연산을 거꾸로 해준 것이라고 볼 수 있습니다. Interpolation은 보간법으로 주어진 값들을 통해 추정해야 하는 픽셀(여기서는 특성 맵의 크기가 커지면서 메꾸어야 하는 중간 픽셀들을 의미합니다.) 추정하는 방법입니다.\n",
    "\n",
    "Upsampling만 하면 원하는 세그멘테이션 맵을 얻을 수 있습니다. 그것이 바로 FCN-32s의 경우입니다.\n",
    "\n",
    "하지만 논문에서는 더 나은 성능을 위해서 한 가지 기법을 더해줍니다. 위 그림에서 확인할 수 있는 Skip Architecture라는 방법입니다. 논문에서는 FCN-32s, FCN-16s, FCN-8s로 결과를 구분해 설명합니다. FCN-16s는 앞쪽 블록에서 얻은 예측 결과 맵과, 2배로 upsampling한 맵을 더한 후, 한 번에 16배로 upsampling을 해주어 얻습니다. 여기서 한 번 더 앞쪽 블록을 사용하면 FCN-8s를 얻을 수 있습니다. 이 기법으로 나아지는 결과는 논문 원문에 소개된 아래 그림에서 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-twist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-worker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-vitamin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-viking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-screw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-london",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-found",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-contest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-absence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-lightning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-polish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-angel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-bahamas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-technician",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
