{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅎ8!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러리를 로드합니다. \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.backend import int_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import  HorizontalFlip, RandomSizedCrop, Compose, OneOf, Resize\n",
    "\n",
    "def build_augmentation(is_train=True):\n",
    "  if is_train:    # 훈련용 데이터일 경우\n",
    "    return Compose([\n",
    "                    HorizontalFlip(p=0.5),    # 50%의 확률로 좌우대칭\n",
    "                    RandomSizedCrop(         # 50%의 확률로 RandomSizedCrop\n",
    "                        min_max_height=(300, 370),\n",
    "                        w2h_ratio=370/1242,\n",
    "                        height=224,\n",
    "                        width=224,\n",
    "                        p=0.5\n",
    "                        ),\n",
    "                    Resize(              # 입력이미지를 224X224로 resize\n",
    "                        width=224,\n",
    "                        height=224\n",
    "                        )\n",
    "                    ])\n",
    "  return Compose([      # 테스트용 데이터일 경우에는 224X224로 resize만 수행합니다. \n",
    "                Resize(\n",
    "                    width=224,\n",
    "                    height=224\n",
    "                    )\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getenv('HOME')+'/aiffel/semantic_segmentation/data/training'\n",
    "\n",
    "augmentation = build_augmentation()\n",
    "input_images = glob(os.path.join(dir_path, \"image_2\", \"*.png\"))\n",
    "\n",
    "# 훈련 데이터셋에서 5개만 가져와 augmentation을 적용해 봅시다.  \n",
    "plt.figure(figsize=(12, 20))\n",
    "for i in range(5):\n",
    "    image = imread(input_images[i]) \n",
    "    image_data = {\"image\":image}\n",
    "    resized = augmentation(**image_data, is_train=False)\n",
    "    processed = augmentation(**image_data)\n",
    "    plt.subplot(5, 2, 2*i+1)\n",
    "    plt.imshow(resized[\"image\"])  # 왼쪽이 원본이미지\n",
    "    plt.subplot(5, 2, 2*i+2)\n",
    "    plt.imshow(processed[\"image\"])  # 오른쪽이 augment된 이미지\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiGenerator(tf.keras.utils.Sequence):\n",
    "  '''\n",
    "  KittiGenerator는 tf.keras.utils.Sequence를 상속받습니다.\n",
    "  우리가 KittiDataset을 원하는 방식으로 preprocess하기 위해서 Sequnce를 커스텀해 사용합니다.\n",
    "  '''\n",
    "  def __init__(self, \n",
    "               dir_path,\n",
    "               batch_size=4,\n",
    "               img_size=(224, 224, 3),\n",
    "               output_size=(224, 224),\n",
    "               is_train=True,\n",
    "               augmentation=None):\n",
    "    '''\n",
    "    dir_path: dataset의 directory path입니다.\n",
    "    batch_size: batch_size입니다.\n",
    "    img_size: preprocess에 사용할 입력이미지의 크기입니다.\n",
    "    output_size: ground_truth를 만들어주기 위한 크기입니다.\n",
    "    is_train: 이 Generator가 학습용인지 테스트용인지 구분합니다.\n",
    "    augmentation: 적용하길 원하는 augmentation 함수를 인자로 받습니다.\n",
    "    '''\n",
    "    self.dir_path = dir_path\n",
    "    self.batch_size = batch_size\n",
    "    self.is_train = is_train\n",
    "    self.dir_path = dir_path\n",
    "    self.augmentation = augmentation\n",
    "    self.img_size = img_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    # load_dataset()을 통해서 kitti dataset의 directory path에서 라벨과 이미지를 확인합니다.\n",
    "    self.data = self.load_dataset()\n",
    "\n",
    "  def load_dataset(self):\n",
    "    # kitti dataset에서 필요한 정보(이미지 경로 및 라벨)를 directory에서 확인하고 로드하는 함수입니다.\n",
    "    # 이때 is_train에 따라 test set을 분리해서 load하도록 해야합니다.\n",
    "    input_images = glob(os.path.join(self.dir_path, \"image_2\", \"*.png\"))\n",
    "    label_images = glob(os.path.join(self.dir_path, \"semantic\", \"*.png\"))\n",
    "    input_images.sort()\n",
    "    label_images.sort()\n",
    "    assert len(input_images) == len(label_images)\n",
    "    data = [ _ for _ in zip(input_images, label_images)]\n",
    "\n",
    "    if self.is_train:\n",
    "      return data[:-30]\n",
    "    return data[-30:]\n",
    "    \n",
    "  def __len__(self):\n",
    "    # Generator의 length로서 전체 dataset을 batch_size로 나누고 소숫점 첫째자리에서 올림한 값을 반환합니다.\n",
    "    return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # 입력과 출력을 만듭니다.\n",
    "    # 입력은 resize및 augmentation이 적용된 input image이고 \n",
    "    # 출력은 semantic label입니다.\n",
    "    batch_data = self.data[\n",
    "                           index*self.batch_size:\n",
    "                           (index + 1)*self.batch_size\n",
    "                           ]\n",
    "    inputs = np.zeros([self.batch_size, *self.img_size])\n",
    "    outputs = np.zeros([self.batch_size, *self.output_size])\n",
    "        \n",
    "    for i, data in enumerate(batch_data):\n",
    "      input_img_path, output_path = data\n",
    "      _input = imread(input_img_path)\n",
    "      _output = imread(output_path)\n",
    "      _output = (_output==7).astype(np.uint8)*1\n",
    "      data = {\n",
    "          \"image\": _input,\n",
    "          \"mask\": _output,\n",
    "          }\n",
    "      augmented = self.augmentation(**data)\n",
    "      inputs[i] = augmented[\"image\"]/255\n",
    "      outputs[i] = augmented[\"mask\"]\n",
    "      return inputs, outputs\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    # 한 epoch가 끝나면 실행되는 함수입니다. 학습중인 경우에 순서를 random shuffle하도록 적용한 것을 볼 수 있습니다.\n",
    "    self.indexes = np.arange(len(self.data))\n",
    "    if self.is_train == True :\n",
    "      np.random.shuffle(self.indexes)\n",
    "      return self.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_shape(target, refer):\n",
    "    # width, the 3rd dimension\n",
    "    cw = target[2] - refer[2]\n",
    "    assert (cw >= 0)\n",
    "    if cw % 2 != 0:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "    else:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2)\n",
    "    # height, the 2nd dimension\n",
    "    ch = target[1] - refer[1]\n",
    "    assert (ch >= 0)\n",
    "    if ch % 2 != 0:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "    else:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "    return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = build_augmentation()\n",
    "test_preproc = build_augmentation(is_train=False)\n",
    "        \n",
    "train_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=augmentation,\n",
    ")\n",
    "\n",
    "test_generator = KittiGenerator(\n",
    "    dir_path, \n",
    "    augmentation=test_preproc,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unet_model(input_shape=(224, 224, 3), filters= [64, 128, 256, 512, 1024], kernel=3):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b1_conv1')(inputs)\n",
    "    b1 = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b1_conv2')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2,2), name='b2_pool')(b1)\n",
    "    x = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b2_conv1')(x)\n",
    "    b2 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b2_conv2')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2,2), name='b3_pool')(b2)\n",
    "    x = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b3_conv1')(x)\n",
    "    b3 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b3_conv2')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2,2), name='b4_pool')(b3)\n",
    "    x = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b4_conv1')(x)\n",
    "    b4 = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b4_conv2')(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2,2), name='b5_pool')(b4)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(filters[4], kernel, activation='relu', padding='same',kernel_initializer='he_normal', name='b5_conv1')(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding='valid', name='u1_convTran')(x)\n",
    "    ch, cw = get_crop_shape(int_shape(b4), int_shape(x))\n",
    "    r1 = Cropping2D(cropping=(ch, cw), name='u1_resize')(b4)\n",
    "    x = concatenate([r1,x], axis = 3, name='u1_merge')\n",
    "    x = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u1_conv1')(x)\n",
    "    x = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u1_conv2')(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u2_convTran')(x)\n",
    "    ch, cw = get_crop_shape(int_shape(b3), int_shape(x))\n",
    "    r2 = Cropping2D(cropping=(ch, cw), name='u2_resize')(b3)\n",
    "    x = concatenate([r2,x], axis = 3, name='u2_merge')\n",
    "    x = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u2_conv1')(x)\n",
    "    x = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u2_conv2')(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), padding='valid', name='u3_convTran')(x)\n",
    "    ch, cw = get_crop_shape(int_shape(b2), int_shape(x))\n",
    "    r3 = Cropping2D(cropping=(ch, cw), name='u3_resize')(b2)\n",
    "    x = concatenate([r3,x], axis = 3, name='u3_merge')\n",
    "    x = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u3_conv1')(x)\n",
    "    x = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u3_conv2')(x)\n",
    "    \n",
    "    x = Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), padding='valid', name='u4_convTran')(x)\n",
    "    ch, cw = get_crop_shape(int_shape(b1), int_shape(x))\n",
    "    r4 = Cropping2D(cropping=(ch, cw ), name='u4_resize')(b1)\n",
    "    x = concatenate([r4,x], axis = 3, name='u4_merge')\n",
    "    x = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u4_conv1')(x)\n",
    "    x = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='u4_conv2')(x)\n",
    "    x = Conv2D(2, kernel, activation='relu', padding='same',kernel_initializer='he_normal', name='u4_conv3')(x)\n",
    "    x = Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    output = tf.image.resize(x, (inputs.shape[1], inputs.shape[2]), name='output')\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_model = Unet_model()\n",
    "u_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n",
    "history = u_model.fit_generator(\n",
    "     generator=train_generator,\n",
    "     validation_data=test_generator,\n",
    "     steps_per_epoch=len(train_generator),\n",
    "     epochs=100,\n",
    " )\n",
    "\n",
    "model_path = dir_path + '/seg_model_unet.h5'\n",
    "u_model.save(model_path)  #학습한 모델을 저장해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Upp_model(input_shape=(224, 224, 3), filters= [0, 64, 128, 256, 512], kernel=3):\n",
    "    inputs = Input(input_shape)\n",
    "#     x0_0 = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b1_conv1')(inputs)\n",
    "#     x0_0 = Conv2D(filters[0], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='b1_conv2')(x0_0)\n",
    "    \n",
    "    x1_0 = MaxPooling2D(pool_size=(2,2), name='x1_0_pool')(inputs)\n",
    "    x1_0 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_0_conv1')(inputs)\n",
    "    x1_0 = BatchNormalization()(x1_0)\n",
    "    x1_0 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_0_conv2')(x1_0)\n",
    "    x1_0 = BatchNormalization()(x1_0)\n",
    "    \n",
    "    x2_0 = MaxPooling2D(pool_size=(2,2), name='x2_0_pool')(x1_0)\n",
    "    x2_0 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_0_conv1')(x2_0)\n",
    "    x2_0 = BatchNormalization()(x2_0)\n",
    "    x2_0 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_0_conv2')(x2_0)\n",
    "    x2_0 = BatchNormalization()(x2_0)\n",
    "    \n",
    "    x3_0 = MaxPooling2D(pool_size=(2,2), name='x3_0_pool')(x2_0)\n",
    "    x3_0 = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x3_0_conv1')(x3_0)\n",
    "    x3_0 = BatchNormalization()(x3_0)\n",
    "    x3_0 = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x3_0_conv2')(x3_0)\n",
    "    x3_0 = BatchNormalization()(x3_0)\n",
    "    \n",
    "    x4_0 = MaxPooling2D(pool_size=(2,2), name='x4_0_pool')(x3_0)\n",
    "    x4_0 = Conv2D(filters[4], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x4_0_conv1')(x4_0)\n",
    "    x4_0 = BatchNormalization()(x4_0)\n",
    "    x4_0 = Conv2D(filters[4], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x4_0_conv2')(x4_0)\n",
    "    x4_0 = BatchNormalization()(x4_0)\n",
    "    \n",
    "    u4_0 = Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding='valid', name='u4_0_convTran')(x4_0)\n",
    "    r3_0 = tf.image.resize(x3_0, (u4_0.shape[1], u4_0.shape[2]), name='x3_0_resize')\n",
    "    x3_1 = concatenate([r3_0, u4_0], axis = 3, name='x3_1_merge')\n",
    "    x3_1 = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x3_1_conv1')(x3_1)\n",
    "    x3_1 = Conv2D(filters[3], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x3_1_conv2')(x3_1)\n",
    "    \n",
    "    # 2-2로 감\n",
    "    u3_0 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u3_0_convTran')(x3_0)\n",
    "    r2_0 = tf.image.resize(x2_0, (u3_0.shape[1], u3_0.shape[2]), name='a_r2_0_resize')\n",
    "    x2_1 = concatenate([r2_0,u3_0], axis = 3, name='x2_1_merge')\n",
    "    x2_1 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_1_conv1')(x2_1)\n",
    "    x2_1 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_1_conv2')(x2_1)\n",
    "    \n",
    "    # origin\n",
    "    u3_1 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u3_1_convTran')(x3_1)\n",
    "    r2_0 = tf.image.resize(x2_0, (u3_1.shape[1], u3_1.shape[2]), name='b_r2_0_resize')\n",
    "    r2_1 = tf.image.resize(x2_1, (u3_1.shape[1], u3_1.shape[2]), name='b_r2_1_resize')\n",
    "    x2_2 = concatenate([r2_0,r2_1, u3_1], axis = 3, name='x2_2_merge')\n",
    "    x2_2 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_2_conv1')(x2_2)\n",
    "    x2_2 = Conv2D(filters[2], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x2_2_conv2')(x2_2)\n",
    "    \n",
    "    # 1-3로 감\n",
    "    u2_0 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u2_0_convTran')(x2_0)\n",
    "    r1_0 = tf.image.resize(x1_0, (u2_0.shape[1], u2_0.shape[2]), name='a_r1_0_resize')\n",
    "    x1_1 = concatenate([r1_0,u2_0], axis = 3, name='x1_1_merge')\n",
    "    x1_1 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_1_conv1')(x1_1)\n",
    "    x1_1 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_1_conv2')(x1_1)\n",
    "    \n",
    "    # 1-3로 감\n",
    "    u2_1 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u2_1_convTran')(x2_1)\n",
    "    r1_0 = tf.image.resize(x1_0, (u2_1.shape[1], u2_1.shape[2]), name='b_r1_0_resize')\n",
    "    r1_1 = tf.image.resize(x1_1, (u2_1.shape[1], u2_1.shape[2]), name='b_r1_1_resize')\n",
    "    x1_2 = concatenate([r1_0, r1_1, u2_1], axis = 3, name='x1_2_merge')\n",
    "    x1_2 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_2_conv1')(x1_2)\n",
    "    x1_2 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_2_conv2')(x1_2)\n",
    "    \n",
    "    # origin\n",
    "    u2_2 = Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding='valid', name='u2_2_convTran')(x2_2)\n",
    "    r1_0 = tf.image.resize(x1_0, (u2_2.shape[1], u2_2.shape[2]), name='c_r1_0_resize')\n",
    "    r1_1 = tf.image.resize(x1_1, (u2_2.shape[1], u2_2.shape[2]), name='c_r1_1_resize')\n",
    "    r1_2 = tf.image.resize(x1_2, (u2_2.shape[1], u2_2.shape[2]), name='c_r1_2_resize')\n",
    "    x1_3 = concatenate([r1_0, r1_1, r1_2, u2_2], axis = 3, name='x1_3_merge')\n",
    "    x1_3 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_3_conv1')(x1_3)\n",
    "    x1_3 = Conv2D(filters[1], kernel, activation='relu', padding='valid',kernel_initializer='he_normal', name='x1_3_conv2')(x1_3)\n",
    "    x1_3 = Conv2D(2, kernel, activation='relu', padding='same',kernel_initializer='he_normal', name='x1_3_conv3')(x1_3)\n",
    "    x1_3 = Conv2D(1, 1, activation='sigmoid')(x1_3)\n",
    "    output = tf.image.resize(x1_3, (inputs.shape[1], inputs.shape[2]), name='output')\n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "upp_model = Upp_model()\n",
    "upp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(upp_model, to_file='./upp_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "upp_model = Upp_model()\n",
    "upp_model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy')\n",
    "upp_model.fit_generator(\n",
    "     generator=train_generator,\n",
    "     validation_data=test_generator,\n",
    "     steps_per_epoch=len(train_generator),\n",
    "     epochs=100,\n",
    " )\n",
    "\n",
    "model_path = dir_path + '/seg_model_uppnet.h5'\n",
    "upp_model.save(model_path)  #학습한 모델을 저장해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = dir_path + '/seg_model_unet.h5'\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path):\n",
    "     origin_img = imread(image_path)\n",
    "     data = {\"image\":origin_img}\n",
    "     processed = preproc(**data)\n",
    "     output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "     output = (output[0].numpy()>0.5).astype(np.uint8).squeeze(-1)*255  #0.5라는 threshold를 변경하면 도로인식 결과범위가 달라집니다.\n",
    "     output = Image.fromarray(output)\n",
    "     background = Image.fromarray(origin_img).convert('RGBA')\n",
    "     output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "     output = Image.blend(background, output, alpha=0.5)\n",
    "     output.show()\n",
    "     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_score(target, prediction):\n",
    "    intersection = np.logical_and(target, prediction)\n",
    "    union = np.logical_or(target, prediction)\n",
    "    iou_score = float(np.sum(intersection)) / float(np.sum(union))\n",
    "    print('IoU : %f' % iou_score )\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path, label_path):\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\":origin_img}\n",
    "    processed = preproc(**data)\n",
    "    output = model(np.expand_dims(processed[\"image\"]/255,axis=0))\n",
    "    output = (output[0].numpy()>=0.5).astype(np.uint8).squeeze(-1)*255  \n",
    "    prediction = output/255   # 도로로 판단한 영역\n",
    "    \n",
    "    output = Image.fromarray(output)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    output = output.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    output = Image.blend(background, output, alpha=0.5)\n",
    "    output.show()  \n",
    "     \n",
    "    if label_path:   \n",
    "        label_img = imread(label_path)\n",
    "        label_data = {\"image\":label_img}\n",
    "        label_processed = preproc(**label_data)\n",
    "        label_processed = label_processed[\"image\"]\n",
    "        target = (label_processed == 7).astype(np.uint8)*1   # 라벨에서 도로로 기재된 영역\n",
    "\n",
    "        return output, prediction, target\n",
    "    else:\n",
    "        return output, prediction, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "output, prediction, target = get_output(\n",
    "     upp_model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'/result_{str(i).zfill(3)}.png',\n",
    "     label_path=dir_path + f'/semantic/00{str(i).zfill(4)}_10.png'\n",
    " )\n",
    "\n",
    "calculate_iou_score(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "output, prediction, target = get_output(\n",
    "     upp_model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'/result_{str(i).zfill(3)}.png',\n",
    "     label_path=dir_path + f'/semantic/00{str(i).zfill(4)}_10.png'\n",
    " )\n",
    "\n",
    "calculate_iou_score(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-spring",
   "metadata": {},
   "source": [
    "channel=2 Conv layer를 사용하는 것만으로 상당히 좋은 성능을 보였습니다.  \n",
    "마지막 chanel을 2개로 만드는 이유는 foreground와 background를 구별하기 위함입니다.  \n",
    "이러한 역할을 빼면 더 성능이 안좋다는 사실을 실험을 통해 확인했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1    # i값을 바꾸면 테스트용 파일이 달라집니다. \n",
    "output, prediction, target = get_output(\n",
    "     upp_model, \n",
    "     test_preproc,\n",
    "     image_path=dir_path + f'/image_2/00{str(i).zfill(4)}_10.png',\n",
    "     output_path=dir_path + f'/result_{str(i).zfill(3)}.png',\n",
    "     label_path=dir_path + f'/semantic/00{str(i).zfill(4)}_10.png'\n",
    " )\n",
    "\n",
    "calculate_iou_score(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-peeing",
   "metadata": {},
   "source": [
    "BatchNormalization을 추가했더니 IoU가 0.896350로 더욱 좋아졌습니다.  \n",
    "그리고 기존의 Unet 보다는 U++Net이 성능이 더 좋아보였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-mother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-stuart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-school",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-startup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-survival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-mandate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-civilization",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
