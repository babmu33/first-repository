{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [E-4] 프로젝트 : 네이버 영화리뷰 감성분석 도전하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 스텝까지는 영문 텍스트의 감정분석을 진행해 보았습니다. 그렇다면 이번에는 한국어 텍스트의 감정분석을 진행해 보면 어떨까요? 오늘 활용할 데이터셋은 네이버 영화의 댓글을 모아 구성된 Naver sentiment movie corpus입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-21 16:29:34--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14628807 (14M) [text/plain]\n",
      "Saving to: ‘ratings_train.txt’\n",
      "\n",
      "ratings_train.txt   100%[===================>]  13.95M  10.4MB/s    in 1.3s    \n",
      "\n",
      "2021-01-21 16:29:36 (10.4 MB/s) - ‘ratings_train.txt’ saved [14628807/14628807]\n",
      "\n",
      "--2021-01-21 16:29:36--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.228.133\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|151.101.228.133|:443... 접속됨.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4893335 (4.7M) [text/plain]\n",
      "Saving to: ‘ratings_test.txt’\n",
      "\n",
      "ratings_test.txt    100%[===================>]   4.67M  6.80MB/s    in 0.7s    \n",
      "\n",
      "2021-01-21 16:29:38 (6.80 MB/s) - ‘ratings_test.txt’ saved [4893335/4893335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "! wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "! mv ratings_*.txt ~/aiffel/sentiment_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m pip install konlpy\n",
    "sudo apt-get install curl git\n",
    "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 데이터 셋을 준비하고  konlpy를 다운받아 두었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 데이터를 살펴보니 영화에 대한 리뷰라는것을 잘 확인 할수가 있네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 데이터로더 구성   \n",
    "\n",
    "실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.   \n",
    "\n",
    "\n",
    "    데이터의 중복 제거   \n",
    "    NaN 결측치 제거   \n",
    "    한국어 토크나이저로 토큰화   \n",
    "    불용어(Stopwords) 제거   \n",
    "    사전word_to_index 구성   \n",
    "    텍스트 스트링을 사전 인덱스 스트링으로 변환   \n",
    "    X_train, y_train, X_test, y_test, word_to_index 리턴   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가공되지 않은 날것의 텍스트 파일을 가공해서 쓰라니 이거 실화 입니까? 여기부터 멘붕이 왔다. 1시간 2시간 시간은 계속흐르고 이게 뭐다냐!!! 그러나 늦은 밤까지 학구열을 불태우는 우리를 발견하신 맘착한 영표퍼실님과 영석펴실님과 바롬 퍼실님 덕분에 맘편하게 복붇을 하게 되어 첫 관문을 겨우겨우 해결 했습니다. 데이터가 친절해야지 콜록콜록..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 모델구성을 위한 데이터 분석 및 가공   \n",
    "\n",
    "\n",
    "    데이터셋 내 문장 길이 분포   \n",
    "    적절한 최대 문장 길이 지정   \n",
    "    keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.969376315021577\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843535456326455\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 최대 길이의 값도 전체 모델 성능에 영향을 미친다니 적절한 값을 찾기 위해 전체 데이터셋 분포를 확인해보니 최대 문장길이가 116이네요. 그리고 padding 방식은 'post'로 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 모델구성 및 validation set 구성   \n",
    "\n",
    "모델은 3가지 이상 다양하게 구성하여 실험해 보세요. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          22416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,024,369\n",
      "Trainable params: 2,024,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Conv1D 모델사용\n",
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 200   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 6688      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,006,769\n",
      "Trainable params: 2,006,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM 모델사용\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 1608      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,001,617\n",
      "Trainable params: 2,001,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D() 레이어 하나만 사용하는 모델\n",
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D 모델, LSTM 모델, GlobalMaxPooling1D() 레이어 하나만 사용하는 모델 세가지를 사용할 계획입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 12000건 분리\n",
    "x_val = X_train[:12000]   \n",
    "y_val = y_train[:12000]\n",
    "\n",
    "# validation set을 제외한 나머지 \n",
    "partial_X_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련용 데이터셋 25000건중 12000건을 분리하여 검증셋(validation set)으로 사용하도록 구성했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) 모델 훈련 개시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 40)          400000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 16)          4496      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 406,449\n",
      "Trainable params: 406,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 40   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "266/266 [==============================] - 9s 32ms/step - loss: 0.4580 - accuracy: 0.7828 - val_loss: 0.3390 - val_accuracy: 0.8550\n",
      "Epoch 2/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.3203 - accuracy: 0.8634 - val_loss: 0.3231 - val_accuracy: 0.8612\n",
      "Epoch 3/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.2864 - accuracy: 0.8815 - val_loss: 0.3134 - val_accuracy: 0.8691\n",
      "Epoch 4/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.2501 - accuracy: 0.8996 - val_loss: 0.3140 - val_accuracy: 0.8676\n",
      "Epoch 5/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.2066 - accuracy: 0.9223 - val_loss: 0.3272 - val_accuracy: 0.8709\n",
      "Epoch 6/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.1597 - accuracy: 0.9426 - val_loss: 0.3557 - val_accuracy: 0.8662\n",
      "Epoch 7/7\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.1192 - accuracy: 0.9594 - val_loss: 0.3965 - val_accuracy: 0.8627\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=7  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.4725 - accuracy: 0.8396\n",
      "[0.4725327491760254, 0.8395955562591553]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv1D모델의  accuracy는  0.8396가 나왔네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 6688      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,006,769\n",
      "Trainable params: 2,006,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM 모델사용\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.5322 - accuracy: 0.7104 - val_loss: 0.3558 - val_accuracy: 0.8488\n",
      "Epoch 2/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.3391 - accuracy: 0.8560 - val_loss: 0.3283 - val_accuracy: 0.8583\n",
      "Epoch 3/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.3030 - accuracy: 0.8726 - val_loss: 0.3182 - val_accuracy: 0.8683\n",
      "Epoch 4/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.2755 - accuracy: 0.8834 - val_loss: 0.3251 - val_accuracy: 0.8665\n",
      "Epoch 5/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.2499 - accuracy: 0.8947 - val_loss: 0.3249 - val_accuracy: 0.8700\n",
      "Epoch 6/7\n",
      "266/266 [==============================] - 7s 27ms/step - loss: 0.2281 - accuracy: 0.9032 - val_loss: 0.3292 - val_accuracy: 0.8717\n",
      "Epoch 7/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.2072 - accuracy: 0.9132 - val_loss: 0.3559 - val_accuracy: 0.8704\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=7  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.4034 - accuracy: 0.8517\n",
      "[0.40344545245170593, 0.8516793251037598]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM 모델의  accuracy은  0.8516이 나왔네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 1608      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,001,617\n",
      "Trainable params: 2,001,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GlobalMaxPooling1D() 레이어 하나만 사용하는 모델\n",
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "266/266 [==============================] - 6s 24ms/step - loss: 0.5227 - accuracy: 0.7577 - val_loss: 0.3436 - val_accuracy: 0.8522\n",
      "Epoch 2/7\n",
      "266/266 [==============================] - 6s 23ms/step - loss: 0.3194 - accuracy: 0.8657 - val_loss: 0.3150 - val_accuracy: 0.8626\n",
      "Epoch 3/7\n",
      "266/266 [==============================] - 6s 23ms/step - loss: 0.2711 - accuracy: 0.8903 - val_loss: 0.3110 - val_accuracy: 0.8677\n",
      "Epoch 4/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.2307 - accuracy: 0.9100 - val_loss: 0.3139 - val_accuracy: 0.8679\n",
      "Epoch 5/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.1907 - accuracy: 0.9294 - val_loss: 0.3279 - val_accuracy: 0.8720\n",
      "Epoch 6/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.1506 - accuracy: 0.9486 - val_loss: 0.3427 - val_accuracy: 0.8730\n",
      "Epoch 7/7\n",
      "266/266 [==============================] - 6s 24ms/step - loss: 0.1127 - accuracy: 0.9650 - val_loss: 0.3647 - val_accuracy: 0.8712\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=7  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.4378 - accuracy: 0.8455\n",
      "[0.43782246112823486, 0.8454543352127075]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GlobalMaxPooling1D 모델의  accuracy도  0.8455이 나왔네요. 어느 모델을 써도 큰차이가 없을듯 하지만  LSTM모델이 조금 높습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Loss, Accuracy 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.4378 - accuracy: 0.8455\n",
      "[0.43782246112823486, 0.8454543352127075]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss, metric을 체크하면 지금 모델이 underfit 되고 있는지, overfit 되고 있는지, 또는 그 외의 문제가 있는지를 알 수 있습니다. 이런 문제점을 체크함으로써 모델의 성능을 높이기 위해 데이터를 추가해야 하는지, feature를 늘려야 하는지, 모델의 크기를 늘려야 하는지 다른 문제를 봐야하는지를 알 수 있죠. 간단하게 말하자면 Train set과 Validation set의 loss가 같이 충분히 떨어지는게 좋습니다. 위 그래프는 validation set loss 에포크2를 기점으로 상승하는 것을 볼수 있습니다. 그럼 이 시점부터 오버피팅 된다는 건데 이게 잘 이해가 안가긴 합니다. 그럼 에포크를 2만 줘도 될까요? 이렇게  짧은 학습만으로 좋은 학습이 될수 있는지 이 AI맹은 이해가 어렵습니다. 나중에 퍼실님들께 물어봐야겠네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습된 Embedding 레이어 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12228241, -0.07429194, -0.10975125, -0.11223913, -0.05160241,\n",
       "       -0.10585713, -0.09726408,  0.35894704, -0.1061317 , -0.10762132,\n",
       "       -0.1025951 , -0.02402776, -0.00867452, -0.0129066 , -0.13015912,\n",
       "       -0.06372445, -0.03926619, -0.12453793, -0.12995067, -0.05241904,\n",
       "       -0.07883701, -0.07072418, -0.11107545, -0.15016775, -0.12751435,\n",
       "        0.10361876, -0.02260922, -0.10209123, -0.10428516, -0.11316857,\n",
       "       -0.08022484, -0.0953861 , -0.15359257, -0.09251904, -0.09876927,\n",
       "       -0.08604755, -0.10492505, -0.12090148, -0.00268767, -0.10618176,\n",
       "       -0.1023434 , -0.09816681, -0.12430054,  0.04685304, -0.13773035,\n",
       "       -0.03033251, -0.00047974, -0.09005254, -0.10240762, -0.06108148,\n",
       "       -0.08139846, -0.11329286, -0.05714973, -0.10019092, -0.11782655,\n",
       "       -0.09992022, -0.06288098, -0.13375235, -0.08894271, -0.08829362,\n",
       "       -0.09009536, -0.12287966, -0.07833181, -0.14076932, -0.1225758 ,\n",
       "       -0.03276773, -0.12622783, -0.02175479, -0.07229403, -0.067254  ,\n",
       "       -0.10790575, -0.04506941, -0.07762948,  0.1998956 , -0.12351409,\n",
       "       -0.09788892, -0.06476804, -0.05991101, -0.05729527, -0.11787185,\n",
       "       -0.01014113,  0.16603409, -0.1008665 , -0.07824181, -0.13167262,\n",
       "       -0.06680706, -0.09450466, -0.09879754, -0.10468903, -0.15277186,\n",
       "       -0.03633469, -0.09477421, -0.04283882, -0.09305997, -0.09201049,\n",
       "       -0.03437741, -0.08753335,  0.00634869, -0.04223973, -0.12986153,\n",
       "       -0.13636577, -0.09989157, -0.09853768, -0.09963942, -0.08794424,\n",
       "       -0.1151033 , -0.00117679,  0.30696505, -0.05876521, -0.10559074,\n",
       "       -0.02023968, -0.09082562, -0.06719738, -0.10254578, -0.09841007,\n",
       "       -0.01625397, -0.10094163, -0.09847686, -0.09432524, -0.10548996,\n",
       "       -0.11536454, -0.08858979, -0.06317976, -0.10435757, -0.10484163,\n",
       "       -0.09184194, -0.09046832, -0.07082169, -0.09216318, -0.12102545,\n",
       "       -0.07375837, -0.0914735 , -0.09347379, -0.10012899, -0.14876889,\n",
       "       -0.04167395, -0.09372097, -0.07256646, -0.08651961,  0.06740288,\n",
       "       -0.11611099, -0.13778557, -0.01534782, -0.07341315, -0.08811958,\n",
       "       -0.15919533, -0.07796856, -0.10449461, -0.10985354, -0.11447724,\n",
       "       -0.07482915, -0.06428243, -0.00529786, -0.09345267, -0.03103614,\n",
       "       -0.064541  , -0.10588421, -0.09504823, -0.10931251, -0.06376643,\n",
       "       -0.03921887, -0.10583191, -0.07330021, -0.09088877, -0.02662969,\n",
       "       -0.07284305, -0.0922996 , -0.09294896, -0.08436909,  0.00222546,\n",
       "       -0.10570813, -0.10622377, -0.09935811,  0.13811988, -0.07704584,\n",
       "       -0.08714879, -0.10932741, -0.10328301,  0.0401543 , -0.00123756,\n",
       "       -0.09493636, -0.13649984, -0.09551087, -0.09413195, -0.09783768,\n",
       "       -0.08398952, -0.08769474, -0.08294438, -0.09657877, -0.07467662,\n",
       "       -0.04313428, -0.0947585 , -0.04249297, -0.10192624, -0.10748239,\n",
       "       -0.07990811, -0.08297893, -0.09156266, -0.04617722, -0.08857697],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['영화']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('원피스', 0.383182168006897),\n",
       " ('서구', 0.36453112959861755),\n",
       " ('놀란', 0.3596607446670532),\n",
       " ('봤', 0.3453797698020935),\n",
       " ('놀라운', 0.34436145424842834),\n",
       " ('시절', 0.34349551796913147),\n",
       " ('된', 0.3374200761318207),\n",
       " ('마스터', 0.3358646035194397),\n",
       " ('던', 0.33457693457603455),\n",
       " ('일대기', 0.33455270528793335)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"눈물\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 임베딩 레이어를 분석하고 눈물이라는 단어를와 유사한 단어를 유도해보니 그렇게  유의미한 학습이 된것 같지는 않네요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) 한국어 Word2Vec 임베딩 활용하여 성능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim import models\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko/ko.bin'\n",
    "ko_model = models.Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj05/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/home/aiffel-dj05/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in ko_model:\n",
    "        embedding_matrix[i] = ko_model[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 8)                 6688      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,006,769\n",
      "Trainable params: 2,006,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그나마 정확도가 조금 높았던 LSTM모델을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0866 - accuracy: 0.9701 - val_loss: 0.5328 - val_accuracy: 0.8635\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0800 - accuracy: 0.9732 - val_loss: 0.5244 - val_accuracy: 0.8630\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0790 - accuracy: 0.9737 - val_loss: 0.5287 - val_accuracy: 0.8619\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0762 - accuracy: 0.9748 - val_loss: 0.5469 - val_accuracy: 0.8614\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0753 - accuracy: 0.9750 - val_loss: 0.5443 - val_accuracy: 0.8577\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0739 - accuracy: 0.9754 - val_loss: 0.5758 - val_accuracy: 0.8612\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0725 - accuracy: 0.9764 - val_loss: 0.5653 - val_accuracy: 0.8578\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0672 - accuracy: 0.9786 - val_loss: 0.5806 - val_accuracy: 0.8583\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0666 - accuracy: 0.9788 - val_loss: 0.5716 - val_accuracy: 0.8595\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0650 - accuracy: 0.9793 - val_loss: 0.6003 - val_accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.7082 - accuracy: 0.8354\n",
      "[0.7082197070121765, 0.8354049324989319]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy가 0.8354가 나왔네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0674 - accuracy: 0.9781 - val_loss: 0.5479 - val_accuracy: 0.8603\n",
      "Epoch 2/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0620 - accuracy: 0.9804 - val_loss: 0.5698 - val_accuracy: 0.8570\n",
      "Epoch 3/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0601 - accuracy: 0.9813 - val_loss: 0.5952 - val_accuracy: 0.8560\n",
      "Epoch 4/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0593 - accuracy: 0.9813 - val_loss: 0.5834 - val_accuracy: 0.8564\n",
      "Epoch 5/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0582 - accuracy: 0.9816 - val_loss: 0.5912 - val_accuracy: 0.8571\n",
      "Epoch 6/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0564 - accuracy: 0.9824 - val_loss: 0.6232 - val_accuracy: 0.8570\n",
      "Epoch 7/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0559 - accuracy: 0.9827 - val_loss: 0.6188 - val_accuracy: 0.8563\n",
      "Epoch 8/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0544 - accuracy: 0.9832 - val_loss: 0.6316 - val_accuracy: 0.8548\n",
      "Epoch 9/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0546 - accuracy: 0.9828 - val_loss: 0.6520 - val_accuracy: 0.8543\n",
      "Epoch 10/15\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 0.6467 - val_accuracy: 0.8535\n",
      "Epoch 11/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0534 - accuracy: 0.9834 - val_loss: 0.6270 - val_accuracy: 0.8583\n",
      "Epoch 12/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0511 - accuracy: 0.9843 - val_loss: 0.6384 - val_accuracy: 0.8560\n",
      "Epoch 13/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0499 - accuracy: 0.9846 - val_loss: 0.6305 - val_accuracy: 0.8568\n",
      "Epoch 14/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0488 - accuracy: 0.9849 - val_loss: 0.6439 - val_accuracy: 0.8558\n",
      "Epoch 15/15\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0464 - accuracy: 0.9857 - val_loss: 0.6565 - val_accuracy: 0.8565\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=15  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.7825 - accuracy: 0.8291\n",
      "[0.7825247645378113, 0.8291189670562744]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0482 - accuracy: 0.9850 - val_loss: 0.6214 - val_accuracy: 0.8567\n",
      "Epoch 2/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0452 - accuracy: 0.9860 - val_loss: 0.6769 - val_accuracy: 0.8553\n",
      "Epoch 3/7\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.0468 - accuracy: 0.9854 - val_loss: 0.6413 - val_accuracy: 0.8552\n",
      "Epoch 4/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0467 - accuracy: 0.9856 - val_loss: 0.6359 - val_accuracy: 0.8572\n",
      "Epoch 5/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0445 - accuracy: 0.9863 - val_loss: 0.6435 - val_accuracy: 0.8557\n",
      "Epoch 6/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0439 - accuracy: 0.9863 - val_loss: 0.6740 - val_accuracy: 0.8543\n",
      "Epoch 7/7\n",
      "266/266 [==============================] - 7s 25ms/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 0.6613 - val_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=7  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 3s - loss: 0.7866 - accuracy: 0.8231\n",
      "[0.7865651845932007, 0.8230770826339722]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy를 높이고 싶어서 에포크 값을 이리저리 바꿔 보았지만 더 떨어집니다 ㅜㅜ 여기까지 하고 번아웃 되버렸습니다. 저는 이제 가망이 없어요 ^^;;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 총평"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몇일동안 이과제를 하느라 밥도 제대로 못먹고, 잠도 제대로 못자고 속이 울렁 거립니다. 무슨 뜻인지 제대로 이해하지 못한 코드가 눈앞에 아른거리고 이과제를 제대로 수행한건지 알수가 없네요 ㅎㅎㅎ. 부디 너그러운 마응을 가지고 평가해 주세요^^ 이번과제 정말 불친절 했습니다. 이번에도 팀과 함께라 해결할수 있었습니다. 함께 머리를 맞대고 고민해 주신 팀들께 감사드려요~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
